{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629\n",
    "# https://www.kaggle.com/rspadim/simple-denoise-autoencoder-with-keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.56)\n",
    "\n",
    "import gc\n",
    "import os, sys, random, math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score, confusion_matrix\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, BatchNormalization, Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from scipy.stats import rankdata, spearmanr\n",
    "from scipy.special import erfinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "temp_train = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/train.csv')\n",
    "print('\\tSuccessfully loaded train!')\n",
    "\n",
    "temp_test = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/test.csv')\n",
    "print('\\tSuccessfully loaded test!')\n",
    "\n",
    "print('Data was successfully loaded!\\n')\n",
    "\n",
    "print(\"Train shape : \"+str(temp_train.shape))\n",
    "print(\"Test shape  : \"+str(temp_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train['loctm'] = (temp_train['loctm']//10000).astype(int)\n",
    "temp_test['loctm'] = (temp_test['loctm']//10000).astype(int)\n",
    "temp_test['fraud_ind'] = np.nan\n",
    "\n",
    "\n",
    "temp_train['flbmk'] = temp_train['flbmk'].fillna('Nan')\n",
    "temp_train['flg_3dsmk'] = temp_train['flg_3dsmk'].fillna('Nan')\n",
    "\n",
    "temp_test['flbmk'] = temp_test['flbmk'].fillna('Nan')\n",
    "temp_test['flg_3dsmk'] = temp_test['flg_3dsmk'].fillna('Nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merging test and train')\n",
    "temp_train = temp_train.append(temp_test).reset_index() # merge train and test\n",
    "del temp_test\n",
    "print('Done, shape=',np.shape(temp_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_gauss(x):\n",
    "    N = x.shape[0]\n",
    "    temp = x.argsort()\n",
    "    rank_x = temp.argsort() / N\n",
    "    rank_x -= rank_x.mean()\n",
    "    rank_x *= 2\n",
    "    efi_x = erfinv(rank_x)\n",
    "    efi_x -= efi_x.mean()\n",
    "    return efi_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.special import erfinv\n",
    "categorical_features = [\n",
    "                        'cano', 'bacno', 'acqic', 'mchno', 'mcc', 'stocn','scity','csmcu',\n",
    "                        'contp', 'etymd', 'hcefg', 'iterm', 'stscd',\n",
    "                        'loctm', 'locdt'\n",
    "                       ]\n",
    "#for col in categorical_features:\n",
    "#    min_max_scaler = MinMaxScaler()\n",
    "#    temp_train[col] = min_max_scaler.fit_transform(temp_train[col].values.reshape(-1,1))\n",
    "for col in categorical_features:\n",
    "    temp_train[col] = rank_gauss(temp_train[col].values)\n",
    "\n",
    "binary_features = [\n",
    "                   'ecfg','flbmk','flg_3dsmk','insfg','ovrlt',\n",
    "                  ]\n",
    "for col in binary_features:\n",
    "    temp_train[col] = temp_train[col].map({'Y':1, 'N':-1, 'Nan':0})\n",
    "\n",
    "continuous_features = ['conam',]\n",
    "for column in continuous_features:\n",
    "    scaler = StandardScaler()\n",
    "    if temp_train[column].max() > 100 and temp_train[column].min() >= 0:\n",
    "        temp_train[column] = np.log1p(temp_train[column])\n",
    "    scaler.fit(temp_train[col].values.reshape(-1,1))\n",
    "    temp_train[column] = scaler.transform(temp_train[column].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "class ReadWriteLock:\n",
    "    def __init__(self):\n",
    "        self._read_ready = threading.Condition(threading.Lock())\n",
    "        self._readers = 0\n",
    "    def acquire_read(self):\n",
    "        self._read_ready.acquire()\n",
    "        try:\n",
    "            self._readers += 1\n",
    "        finally:\n",
    "            self._read_ready.release()\n",
    "    def release_read(self):\n",
    "        self._read_ready.acquire()\n",
    "        try:\n",
    "            self._readers -= 1\n",
    "            if not self._readers:\n",
    "                self._read_ready.notifyAll()\n",
    "        finally:\n",
    "            self._read_ready.release()\n",
    "    def acquire_write(self):\n",
    "        self._read_ready.acquire()\n",
    "        while self._readers > 0:\n",
    "            self._read_ready.wait()\n",
    "    def release_write(self):\n",
    "        self._read_ready.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from keras.utils import Sequence\n",
    "class DAESequence(Sequence):\n",
    "    def __init__(self, df, batch_size=128, random_cols=.15, random_rows=1, use_cache=False, use_lock=False, verbose=True): #batch_size=128\n",
    "        self.df = df.values.copy()     # ndarray baby\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.len_data = df.shape[0]\n",
    "        self.len_input_columns = df.shape[1]\n",
    "        if(random_cols <= 0):\n",
    "            self.random_cols = 0\n",
    "        elif(random_cols >= 1):\n",
    "            self.random_cols = self.len_input_columns\n",
    "        else:\n",
    "            self.random_cols = int(random_cols*self.len_input_columns)\n",
    "        if(self.random_cols > self.len_input_columns):\n",
    "            self.random_cols = self.len_input_columns\n",
    "        self.random_rows = random_rows\n",
    "        self.cache = None\n",
    "        self.use_cache = use_cache\n",
    "        self.use_lock = use_lock\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.lock = ReadWriteLock()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if(not self.use_cache):\n",
    "            return\n",
    "        if(self.use_lock):\n",
    "            self.lock.acquire_write()\n",
    "        if(self.verbose):\n",
    "            print(\"Doing Cache\")\n",
    "        self.cache = {}\n",
    "        for i in range(0, self.__len__()):\n",
    "            self.cache[i] = self.__getitem__(i, True)\n",
    "        if(self.use_lock):\n",
    "            self.lock.release_write()\n",
    "        gc.collect()\n",
    "        if(self.verbose):\n",
    "            print(\"Done\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(self.len_data / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx, doing_cache=False):\n",
    "        if(not doing_cache and self.cache is not None and not (self.random_cols <=0 or self.random_rows<=0)):\n",
    "            if(idx in self.cache.keys()):\n",
    "                if(self.use_lock):\n",
    "                    self.lock.acquire_read()\n",
    "                ret0, ret1 = self.cache[idx][0], self.cache[idx][1]\n",
    "                if(self.use_lock):\n",
    "                    self.lock.release_read()\n",
    "                if (not doing_cache and self.verbose):\n",
    "                    print('DAESequence Cache ', idx)\n",
    "                return ret0, ret1\n",
    "        idx_end = min(idx + self.batch_size, self.len_data)\n",
    "        cur_len = idx_end - idx\n",
    "        rows_to_sample = int(self.random_rows * cur_len)\n",
    "        input_x = self.df[idx: idx_end]\n",
    "        if (self.random_cols <= 0 or self.random_rows <= 0 or rows_to_sample<=0):\n",
    "            return input_x, input_x\n",
    "\n",
    "        random_rows = np.random.randint(low=0, high=self.len_data-rows_to_sample, size=rows_to_sample)\n",
    "        random_rows[random_rows>idx] += cur_len # just to don't select twice the current rows\n",
    "        cols_to_shuffle = np.random.randint(low=0, high=self.len_input_columns, size=self.random_cols)\n",
    "        noise_x = input_x.copy()\n",
    "        noise_x[0:rows_to_sample, cols_to_shuffle] = self.df[random_rows[:,None], cols_to_shuffle]\n",
    "        if(not doing_cache and self.verbose):\n",
    "            print('DAESequence ', idx)\n",
    "        return noise_x, input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "print(\"Create Model\")\n",
    "dae_data = temp_train[temp_train.columns.drop(['index','txkey','fraud_ind'])] # only get \"X\" vector\n",
    "\n",
    "len_input_columns, len_data = dae_data.shape[1], dae_data.shape[0]\n",
    "NUM_GPUS=1\n",
    "\n",
    "kernel_initializer_1 = keras.initializers.RandomUniform(-math.sqrt(1.0/(len_input_columns*10)),math.sqrt(1.0/(len_input_columns*10)))\n",
    "kernel_initializer_0 = keras.initializers.RandomUniform(-math.sqrt(1.0/(len_input_columns)),math.sqrt(1.0/(len_input_columns)))\n",
    "\n",
    "print(\"Input len=\", len_input_columns, len_data)\n",
    "model_dae = Sequential()\n",
    "model_dae.add(Dense(units=len_input_columns*5, activation='relu', name='Hidden1', input_shape=(len_input_columns,), kernel_initializer=kernel_initializer_0))\n",
    "model_dae.add(Dense(units=len_input_columns*5, activation='relu', name='Hidden2', kernel_initializer=kernel_initializer_1))\n",
    "model_dae.add(Dense(units=len_input_columns*5, activation='relu', name='Hidden3', kernel_initializer=kernel_initializer_1))\n",
    "model_dae.add(Dense(units=len_input_columns, activation='linear', name='Output', kernel_initializer=kernel_initializer_1))\n",
    "model_opt = keras.optimizers.SGD(lr=0.003, decay=1-0.995, momentum=0, nesterov=False)\n",
    "\n",
    "try:\n",
    "    print('Loading model from file')\n",
    "    model_dae = keras.models.load_model('DAE.keras.model.h5')\n",
    "except Exception as e:\n",
    "    print(\"Can't load previous fitting parameters and model\", repr(e))\n",
    "if(NUM_GPUS>1):\n",
    "    try:\n",
    "        multi_gpu_model = keras.utils.multi_gpu_model(model_dae, gpus=NUM_GPUS)\n",
    "        multi_gpu_model.compile(loss='mean_squared_error', optimizer=model_opt) #model_opt Nadam()\n",
    "        print(\"MULTI GPU MODEL\")\n",
    "        print(multi_gpu_model.summary())\n",
    "    except Exception as e:\n",
    "        print(\"Can't run multi gpu, error=\", repr(e))\n",
    "        model_dae.compile(loss='mean_squared_error', optimizer=model_opt)\n",
    "        NUM_GPUS=0\n",
    "else:\n",
    "    model_dae.compile(loss='mean_squared_error', optimizer=model_opt)\n",
    "\n",
    "print(\"BASE MODEL\")\n",
    "print(model_dae.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "batch_size = 128\n",
    "epochs = 1000\n",
    "multi_process_workers = 1\n",
    "if (NUM_GPUS > 1):\n",
    "    multi_gpu_model.fit_generator(\n",
    "        DAESequence(dae_data, batch_size=batch_size*NUM_GPUS, verbose=False),\n",
    "        steps_per_epoch=int(ceil(dae_data.shape[0]/(batch_size*NUM_GPUS))),\n",
    "        workers=multi_process_workers, use_multiprocessing=True if multi_process_workers>1 else False,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            # keras.callbacks.LambdaCallback(on_epoch_end=lambda x,y: model_dae.save('DAE.keras.model.h5')) # save weights \n",
    "        ])\n",
    "else: # single CPU/GPU\n",
    "    model_dae.fit_generator(\n",
    "        DAESequence(dae_data, batch_size=batch_size, verbose=False),\n",
    "        steps_per_epoch=int(ceil(dae_data.shape[0]/batch_size)),\n",
    "        epochs=epochs,\n",
    "        workers=multi_process_workers, use_multiprocessing=True if multi_process_workers>1 else False,\n",
    "        verbose=1, callbacks=[\n",
    "            # keras.callbacks.LambdaCallback(on_epoch_end=lambda x,y: model_dae.save('DAE.keras.model.h5')) # save weights\n",
    "        ])\n",
    "    \n",
    "#model_dae.save('DAE.keras.model.h5') # save weights\n",
    "plt.hist(model_dae.get_weights(), bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "if (NUM_GPUS > 1):\n",
    "    dae_denoised_data = multi_gpu_model.predict(dae_data)\n",
    "else:\n",
    "    dae_denoised_data = model_dae.predict(dae_data)\n",
    "\n",
    "print(\"DAE MSE from train data: \", mean_squared_error(dae_data, dae_denoised_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAE_new_df=temp_train[['txkey','fraud_ind']].copy()\n",
    "\n",
    "for i in ['1','2','3']:\n",
    "    print('Hidden layer',i)\n",
    "    columns_names = ['Hidden_'+str(i)+'_'+str(l) for l in range(0, len_input_columns*5)]\n",
    "    for l in columns_names:\n",
    "        DAE_new_df[l] = 0 # create columns\n",
    "    intermediate_layer_model = Model(inputs=model_dae.input, outputs=model_dae.get_layer('Hidden' + i).output)\n",
    "    DAE_new_df[columns_names] = intermediate_layer_model.predict(dae_data)\n",
    "\n",
    "print('DONE!')\n",
    "print(DAE_new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for df in [DAE_new_df]:\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "DAE_new_df.to_csv(\"DAE-hidden-features_new.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
