{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.56)\n",
    "import gc\n",
    "\n",
    "import os, sys, random, math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold, GroupKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.feature_selection import RFECV\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "train = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/train.csv', index_col='txkey')\n",
    "print('\\tSuccessfully loaded train!')\n",
    "\n",
    "test = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/test.csv', index_col='txkey')\n",
    "print('\\tSuccessfully loaded test!')\n",
    "\n",
    "print('Data was successfully loaded!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_split(dataframe):\n",
    "    dataframe['locdt_M'] = dataframe['locdt'].map({1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:1,10:1,11:1,12:1,13:1,14:1,15:1,\n",
    "                                                   16:1,17:1,18:1,19:1,20:1,21:1,22:1,23:1,24:1,25:1,26:1,27:1,28:1,29:1,30:1,31:1,\n",
    "                                                   32:2,33:2,34:2,35:2,36:2,37:2,38:2,39:2,40:2,41:2,42:2,43:2,44:2,45:2,\n",
    "                                                   46:2,47:2,48:2,49:2,50:2,51:2,52:2,53:2,54:2,55:2,56:2,57:2,58:2,59:2,60:2,61:2,\n",
    "                                                   62:3,63:3,64:3,65:3,66:3,67:3,68:3,69:3,70:3,71:3,72:3,73:3,74:3,75:3,\n",
    "                                                   76:3,77:3,78:3,79:3,80:3,81:3,82:3,83:3,84:3,85:3,86:3,87:3,88:3,89:3,90:3,91:3,92:3,\n",
    "                                                   93:4,94:4,95:4,96:4,97:4,98:4,99:4,100:4,101:4,102:4,103:4,104:4,105:4,106:4,107:4,\n",
    "                                                   108:4,109:4,110:4,111:4,112:4,113:4,114:4,115:4,116:4,117:4,118:4,119:4,120:4})\n",
    "    \n",
    "    dataframe['locdt_W'] = dataframe['locdt'].map({1:1,2:1,3:1,4:1,5:1,6:1,7:1,\n",
    "                                                   8:2,9:2,10:2,11:2,12:2,13:2,14:2,\n",
    "                                                   15:3,16:3,17:3,18:3,19:3,20:3,21:3,\n",
    "                                                   22:4,23:4,24:4,25:4,26:4,27:4,28:4,\n",
    "                                                   29:5,30:5,31:5,32:5,33:5,34:5,35:5,\n",
    "                                                   36:6,37:6,38:6,39:6,40:6,41:6,42:6,\n",
    "                                                   43:7,44:7,45:7,46:7,47:7,48:7,49:7,\n",
    "                                                   50:8,51:8,52:8,53:8,54:8,55:8,56:8,\n",
    "                                                   57:9,58:9,59:9,60:9,61:9,62:9,63:9,\n",
    "                                                   64:10,65:10,66:10,67:10,68:10,69:10,70:10,\n",
    "                                                   71:11,72:11,73:11,74:11,75:11,76:11,77:11,\n",
    "                                                   78:12,79:12,80:12,81:12,82:12,83:12,84:12,\n",
    "                                                   85:13,86:13,87:13,88:13,89:13,90:13,91:13,\n",
    "                                                   92:14,93:14,94:14,95:14,96:14,97:14,98:14,\n",
    "                                                   99:15,100:15,101:15,102:15,103:15,104:15,105:15,\n",
    "                                                   106:16,107:16,108:16,109:16,110:16,111:16,112:16,\n",
    "                                                   113:17,114:17,115:17,116:17,117:17,118:17,119:17,\n",
    "                                                   120:18})\n",
    "    \n",
    "    dataframe['loct_hour'] = (dataframe['loctm']//10000).astype(int)\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "train = date_time_split(train)\n",
    "test = date_time_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['txn_info'] = train['ecfg'].astype(str)+train['flg_3dsmk'].astype(str)+train['flbmk'].astype(str)\n",
    "test['txn_info'] = test['ecfg'].astype(str)+test['flg_3dsmk'].astype(str)+test['flbmk'].astype(str)\n",
    "\n",
    "cat_columns = ['ecfg', 'flbmk', 'flg_3dsmk', 'insfg', 'ovrlt', 'txn_info']\n",
    "\n",
    "for col in cat_columns:\n",
    "    train[col] = train[col].fillna('unseen_before_label')\n",
    "    test[col]  = test[col].fillna('unseen_before_label')\n",
    "    train[col] = train[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "    \n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train[col]) + list(test[col]))\n",
    "    train[col] = lbl.transform(train[col])\n",
    "    test[col]  = lbl.transform(test[col])\n",
    "\n",
    "train.fillna(-999,inplace = True)\n",
    "test.fillna(-999,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount\n",
    "train['conam_check'] = np.where(train['conam'].isin(test['conam']), 1, 0)\n",
    "test['conam_check']  = np.where(test['conam'].isin(train['conam']), 1, 0)\n",
    "\n",
    "# check cano\n",
    "train['cano_check'] = np.where(train['cano'].isin(test['cano']), 1, 0)\n",
    "test['cano_check']  = np.where(test['cano'].isin(train['cano']), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)\n",
    "temp_train = train.copy()\n",
    "#temp_train.reset_index(inplace=True)\n",
    "temp_test = test.copy()\n",
    "#temp_test.reset_index(inplace=True)\n",
    "\n",
    "feq_cols = ['mchno','stocn','csmcu', 'etymd', 'stscd', 'txn_info']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([temp_train[['cano','conam',col]],temp_test[['cano','conam',col]]]) \n",
    "    \n",
    "    new_col_name = 'cano_'+col+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby(['cano',col])['conam'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_all = pd.concat([temp_train[['locdt','cano','conam']],temp_test[['locdt','cano','conam']]])\n",
    "train_test_all['day_cano_count'] = train_test_all.groupby(['locdt','cano'])['conam'].transform('count')\n",
    "train['day_cano_count'] = train_test_all[:train_len].day_cano_count.tolist()\n",
    "test['day_cano_count'] = train_test_all[train_len:].day_cano_count.tolist()\n",
    "\n",
    "train_test_all = pd.concat([temp_train[['loct_hour','cano','conam']],temp_test[['loct_hour','cano','conam']]])\n",
    "train_test_all['hour_cano_count'] = train_test_all.groupby(['loct_hour','cano'])['conam'].transform('count')\n",
    "train['hour_cano_count'] = train_test_all[:train_len].hour_cano_count.tolist()\n",
    "test['hour_cano_count'] = train_test_all[train_len:].hour_cano_count.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "\n",
    "            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "\n",
    "            del dt_df['temp_min'],dt_df['temp_max']\n",
    "    return dt_df\n",
    "\n",
    "\n",
    "train_test_all = pd.concat([temp_train[['cano','locdt']],temp_test[['cano','locdt']]],ignore_index=True,sort=False)\n",
    "train_test_all.reset_index(inplace=True, drop=True)\n",
    "train_test_all = train_test_all.sort_values('locdt')\n",
    "train_test_all.drop_duplicates('cano',keep='first',inplace=True)\n",
    "\n",
    "train_test_all.set_index('cano',inplace=True)\n",
    "cano_date = train_test_all['locdt'].to_dict()\n",
    "\n",
    "train['first_cano_dt'] = train['cano'].map(cano_date)\n",
    "test['first_cano_dt'] = test['cano'].map(cano_date)\n",
    "\n",
    "train = values_normalization(train, ['locdt_W'], ['first_cano_dt'])\n",
    "test = values_normalization(test, ['locdt_W'], ['first_cano_dt'])\n",
    "\n",
    "del temp_train, temp_test, train_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feq_cols = ['mchno','stocn','csmcu','etymd','stscd','txn_info']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([train[['cano',col]],test[['cano',col]]]) \n",
    "    \n",
    "    new_col_name = col+'_cano'+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby([col])['cano'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feq_cols = ['mchno','stocn','csmcu']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([train[['locdt','cano',col]],test[['locdt','cano',col]]]) \n",
    "    \n",
    "    new_col_name = 'day_'+col+'_cano'+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby(['locdt',col])['cano'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_all = pd.concat([train[['locdt_W','acqic', 'mchno', 'scity','csmcu','conam']],test[['locdt_W','acqic', 'mchno', 'scity','csmcu','conam']]])\n",
    "train_test_all['past_w_same_store'] = train_test_all.groupby(['locdt_W','acqic', 'mchno','scity','csmcu',])['conam'].transform('count')\n",
    "train['past_w_same_store'] = train_test_all[:train_len].past_w_same_store.tolist()\n",
    "test['past_w_same_store'] = train_test_all[train_len:].past_w_same_store.tolist()\n",
    "\n",
    "train['past_w_same_store'] = np.where(train['past_w_same_store']>=10, 10, train['past_w_same_store'])\n",
    "test['past_w_same_store']  = np.where(test['past_w_same_store']>=10, 10, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = np.where(((train['past_w_same_store']>=5)&(train['past_w_same_store']<10)), 5, train['past_w_same_store'])\n",
    "test['past_w_same_store'] = np.where(((test['past_w_same_store']>=5)&(test['past_w_same_store']<10)), 5, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = np.where(((train['past_w_same_store']>1)&(train['past_w_same_store']<5)), 2, train['past_w_same_store'])\n",
    "test['past_w_same_store'] = np.where(((test['past_w_same_store']>1)&(test['past_w_same_store']<5)), 2, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = train['past_w_same_store'].astype(str)\n",
    "test['past_w_same_store'] = test['past_w_same_store'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE\n",
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col],df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype('float32')\n",
    "        print(\"'\"+nm+\"'\",', ',end='')\n",
    "        \n",
    "# LABEL ENCODE\n",
    "def encode_LE(col,train=train,test=test,verbose=True):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "    nm = col\n",
    "    if df_comb.max()>32000: \n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb[len(train):].astype('int16')\n",
    "    del df_comb; x=gc.collect()\n",
    "    if verbose: print(\"'\"+nm+\"'\",', ',end='')\n",
    "\n",
    "# COMBINE FEATURES\n",
    "def encode_CB(col1,col2,df1=train,df2=test):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "    encode_LE(nm,verbose=False)\n",
    "    print(\"'\"+nm+\"'\",', ',end='')\n",
    "\n",
    "# GROUP AGGREGATION MEAN AND STD\n",
    "def encode_AG(main_columns, uids, aggregations=['mean'], train_df=train, test_df=test, \n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-999,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-999,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-999,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "\n",
    "# GROUP AGGREGATION NUNIQUE\n",
    "def encode_AG2(main_columns, uids, train_df=train, test_df=test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(\"'\"+col+'_'+main_column+'_ct'+\"'\",', ',end='')\n",
    "\n",
    "# COUNT ENCODE            \n",
    "def encode_CT(uids,train=train,test=test):\n",
    "    for col in uids:\n",
    "        train[col + '_count_full'] = train[col].map(pd.concat([train[col], test[col]], ignore_index=True).value_counts(dropna=False))\n",
    "        test[col + '_count_full'] = test[col].map(pd.concat([train[col], test[col]], ignore_index=True).value_counts(dropna=False))\n",
    "        print(\"'\"+col+ '_count_full'+\"'\",', ',end='')\n",
    "\n",
    "def encode_TG(uids,train=train,test=test):\n",
    "    for col in uids:\n",
    "        temp_dict = train.groupby([col])['fraud_ind'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n",
    "        temp_dict.index = temp_dict[col].values\n",
    "        temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "    \n",
    "        train[col+'_target_mean'] = train[col].map(temp_dict)\n",
    "        test[col+'_target_mean']  = test[col].map(temp_dict)\n",
    "        print(\"'\"+col+'_target_mean'+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_AG(['conam'],['cano', 'bacno', 'mcc', 'mchno', 'acqic',],['mean','std'],usena=True)\n",
    "\n",
    "encode_FE(train,test,['cano', 'bacno', 'mcc', 'mchno', 'acqic',])\n",
    "\n",
    "encode_FE(train,test,['etymd', 'stscd', 'txn_info', 'contp', 'hcefg', 'iterm',])\n",
    "\n",
    "encode_TG(['etymd', 'stscd', 'txn_info'],train,test)\n",
    "\n",
    "encode_CB('mcc','mchno')\n",
    "encode_CB('scity','stocn')\n",
    "encode_CB('mchno','stocn')\n",
    "#encode_FE(train,test,['mcc_mchno','scity_stocn','mchno_stocn'])\n",
    "encode_CT(['mcc_mchno','scity_stocn','mchno_stocn'],train,test,)\n",
    "\n",
    "encode_CB('csmcu','stscd')\n",
    "encode_CB('acqic','etymd')\n",
    "encode_FE(train,test,['csmcu_stscd','acqic_etymd'])\n",
    "\n",
    "encode_CB('stscd','etymd')\n",
    "encode_CB('txn_info','etymd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['uid'] = train['cano'].astype(str)+'_'+train['bacno'].astype(str)\n",
    "test['uid'] = test['cano'].astype(str)+'_'+test['bacno'].astype(str)\n",
    "\n",
    "train['uid2'] = train['uid'].astype(str)+'_'+train['mchno'].astype(str)\n",
    "test['uid2'] = test['uid'].astype(str)+'_'+test['mchno'].astype(str)\n",
    "\n",
    "encode_FE(train,test,['uid'])\n",
    "encode_AG(['conam'],['uid'],['mean','std'],usena=True)\n",
    "\n",
    "encode_AG(['mcc_mchno','scity_stocn','mchno_stocn'],['uid'],['mean','std'],usena=True)\n",
    "encode_AG(['etymd','stscd','txn_info'],['uid'],['mean'],fillna=True,usena=True)\n",
    "encode_AG(['stscd_etymd','txn_info_etymd'],['uid'],['std'],fillna=True,usena=True)\n",
    "\n",
    "encode_AG2(['conam', 'scity_stocn', 'mcc_mchno', 'mchno_stocn'], ['uid'], train_df=train, test_df=test)\n",
    "\n",
    "encode_FE(train,test,['uid2'])\n",
    "encode_AG(['conam'],['uid2'],['mean','std'],usena=True)\n",
    "\n",
    "encode_AG(['csmcu_stscd','acqic_etymd'],['uid2'],['mean','std'],usena=True)\n",
    "encode_AG2(['csmcu_stscd', 'acqic_etymd'], ['uid2'], train_df=train, test_df=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['conam'] = np.log(train['conam'])\n",
    "#test['conam'] = np.log(test['conam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA, FastICA, SparsePCA, KernelPCA, TruncatedSVD\n",
    "\n",
    "cat_cols = ['cano', 'bacno', 'mcc', 'mchno', 'acqic', 'stocn', 'scity', 'csmcu',\n",
    "            'etymd', 'stscd', 'txn_info', 'contp', 'hcefg', 'iterm', 'ovrlt']\n",
    "n_comp = 5\n",
    "\n",
    "temp_train = train[cat_cols].copy()\n",
    "temp_test = test[cat_cols].copy()\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica_results_train = ica.fit_transform(temp_train)\n",
    "ica_results_test = ica.transform(temp_test)\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "tsvd_results_train = tsvd.fit_transform(temp_train)\n",
    "tsvd_results_test = tsvd.transform(temp_test)\n",
    "\n",
    "for i in range(1, n_comp + 1):\n",
    "\n",
    "    train['tSVD_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tSVD_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica_results_test[:, i - 1]\n",
    "    \n",
    "\n",
    "del ica_results_train, ica_results_test, tsvd_results_train, tsvd_results_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_comp = 5\n",
    "# SparsePCA\n",
    "spca = SparsePCA(n_components=n_comp, random_state=42)\n",
    "spca_results_train = spca.fit_transform(temp_train)\n",
    "spca_results_test = spca.transform(temp_test)\n",
    "\n",
    "for i in range(1, n_comp + 1):\n",
    "    \n",
    "    train['spca_' + str(i)] = spca_results_train[:, i - 1]\n",
    "    test['spca_' + str(i)] = spca_results_test[:, i - 1]\n",
    "\n",
    "del temp_train, temp_test, spca_results_train, spca_results_test,\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaEncoder(object):\n",
    "        \n",
    "    def __init__(self, group):\n",
    "        \n",
    "        self.group = group\n",
    "        self.stats = None\n",
    "        \n",
    "    # get counts from df\n",
    "    def fit(self, df, target_col):\n",
    "        self.prior_mean = np.mean(df[target_col])\n",
    "        stats = df[[target_col, self.group]].groupby(self.group)\n",
    "        stats = stats.agg(['sum', 'count'])[target_col]    \n",
    "        stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n",
    "        stats.reset_index(level=0, inplace=True)           \n",
    "        self.stats = stats\n",
    "        \n",
    "    # extract posterior statistics\n",
    "    def transform(self, df, stat_type, N_min=1):\n",
    "        \n",
    "        df_stats = pd.merge(df[[self.group]], self.stats, how='left')\n",
    "        n = df_stats['n'].copy()\n",
    "        N = df_stats['N'].copy()\n",
    "        \n",
    "        # fill in missing\n",
    "        nan_indexs = np.isnan(n)\n",
    "        n[nan_indexs] = self.prior_mean\n",
    "        N[nan_indexs] = 1.0\n",
    "        \n",
    "        # prior parameters\n",
    "        N_prior = np.maximum(N_min-N, 0)\n",
    "        alpha_prior = self.prior_mean*N_prior\n",
    "        beta_prior = (1-self.prior_mean)*N_prior\n",
    "        \n",
    "        # posterior parameters\n",
    "        alpha = alpha_prior + n\n",
    "        beta =  beta_prior + N-n\n",
    "        \n",
    "        # calculate statistics\n",
    "        if stat_type=='mean':\n",
    "            num = alpha\n",
    "            dem = alpha+beta\n",
    "                    \n",
    "        elif stat_type=='mode':\n",
    "            num = alpha-1\n",
    "            dem = alpha+beta-2\n",
    "            \n",
    "        elif stat_type=='median':\n",
    "            num = alpha-1/3\n",
    "            dem = alpha+beta-2/3\n",
    "        \n",
    "        elif stat_type=='var':\n",
    "            num = alpha*beta\n",
    "            dem = (alpha+beta)**2*(alpha+beta+1)\n",
    "                    \n",
    "        elif stat_type=='skewness':\n",
    "            num = 2*(beta-alpha)*np.sqrt(alpha+beta+1)\n",
    "            dem = (alpha+beta+2)*np.sqrt(alpha*beta)\n",
    "\n",
    "        elif stat_type=='kurtosis':\n",
    "            num = 6*(alpha-beta)**2*(alpha+beta+1) - alpha*beta*(alpha+beta+2)\n",
    "            dem = alpha*beta*(alpha+beta+2)*(alpha+beta+3)\n",
    "            \n",
    "        # replace missing\n",
    "        value = num/dem\n",
    "        value[np.isnan(value)] = np.nanmedian(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['mchno','scity','acqic']\n",
    "\n",
    "N_min = 1000  \n",
    "# encode variables\n",
    "for col in cat_columns:\n",
    "    all_data = pd.concat([train[col],test[col]])\n",
    "    N_min = (len(all_data.unique()))*0.1\n",
    "    # fit encoder\n",
    "    be = BetaEncoder(col)\n",
    "    be.fit(train, 'fraud_ind')\n",
    "\n",
    "    # mean\n",
    "    train[col] = be.transform(train, 'mean', N_min)\n",
    "    test[col]  = be.transform(test,  'mean', N_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop noise data\n",
    "cols_to_drop = ['loctm', 'locdt', 'uid', 'uid2', 'insfg',\n",
    "                'first_cano_dt', 'first_cano_dt_locdt_W_min_max', \n",
    "                'locdt_W', 'locdt_M',]\n",
    "\n",
    "print('{} features are going to be dropped for being useless'.format(len(cols_to_drop)))\n",
    "\n",
    "train = train.drop(cols_to_drop, axis=1)\n",
    "test = test.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = [col for col in list(train)]\n",
    "features_columns.remove('fraud_ind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in features_columns:\n",
    "    if train[col].dtype=='O':\n",
    "        print(col)\n",
    "        train[col] = train[col].fillna('unseen_before_label')\n",
    "        test[col]  = test[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train[col] = train[col].astype(str)\n",
    "        test[col] = test[col].astype(str)\n",
    "        \n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(list(train[col])+list(test[col]))\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col]  = le.transform(test[col])\n",
    "    \n",
    "    else:\n",
    "        train[col] = train[col].astype(float).fillna(-999)\n",
    "        test[col]  = test[col].astype(float).fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['fraud_ind']\n",
    "X = train.drop(['fraud_ind'], axis=1)\n",
    "\n",
    "X_test = test\n",
    "\n",
    "print (\"Size of y_train data : {}\" .format(y.shape))\n",
    "print (\"Size of X_train data : {}\" .format(X.shape))\n",
    "print (\"Size of test data : {}\" .format(X_test.shape))\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators' : 3000,\n",
    "          'objective' : 'binary',\n",
    "          'random_state' : SEED,\n",
    "          'subsample' : 0.7,\n",
    "          'colsample_bytree' : 0.7,\n",
    "          'learning_rate' : 0.05,\n",
    "          'importance_type' : 'gain',\n",
    "          'max_depth' : -1,\n",
    "          'num_leaves' : 2**8,\n",
    "          'min_child_samples' : 20,\n",
    "          'min_split_gain' : 0.001,\n",
    "          'bagging_freq' : 1,\n",
    "          'reg_alpha' : 0,\n",
    "          'reg_lambda' : 0,\n",
    "          'n_jobs' : -1,\n",
    "          'device' : 'gpu',\n",
    "          'metric' : 'auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(**params)\n",
    "rfe = RFECV(estimator=clf, step=10, cv=KFold(n_splits=3, shuffle=False), scoring='roc_auc', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal number of features:', rfe.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score\")\n",
    "plt.plot(range(1, len(rfe.grid_scores_) + 1), rfe.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X.columns[rfe.ranking_ == 1]:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators' : 3000,\n",
    "          'objective' : 'binary',\n",
    "          'random_state' : SEED,\n",
    "          'subsample' : 0.7,\n",
    "          'colsample_bytree' : 0.7,\n",
    "          'learning_rate' : 0.05,\n",
    "          'importance_type' : 'gain',\n",
    "          'max_depth' : -1,\n",
    "          'num_leaves' : 2**8,\n",
    "          'min_child_samples' : 20,\n",
    "          'min_split_gain' : 0.001,\n",
    "          'bagging_freq' : 1,\n",
    "          'reg_alpha' : 0,\n",
    "          'reg_lambda' : 0,\n",
    "          'n_jobs' : -1,\n",
    "          'device' : 'gpu',\n",
    "          'metric' : 'None',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from IPython.display import display\n",
    "\n",
    "SEED = 42\n",
    "NFOLDS = 3\n",
    "folds = KFold(n_splits=NFOLDS)\n",
    "columns = [col for col in list(train)]\n",
    "columns.remove('fraud_ind')\n",
    "columns.remove('locdt')\n",
    "splits = folds.split(train, train['fraud_ind'])\n",
    "FEATURES = columns\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    X_train, X_valid = train[columns].iloc[train_index], train[columns].iloc[valid_index]\n",
    "    y_train, y_valid = train['fraud_ind'].iloc[train_index], train['fraud_ind'].iloc[valid_index]\n",
    "    \n",
    "    \n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    clf.fit(X_train, \n",
    "            y_train,\n",
    "            verbose=200, early_stopping_rounds=500, eval_metric = \"auc\",\n",
    "            eval_set=[(X_valid, y_valid)])\n",
    "    \n",
    "    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importances_\n",
    "    \n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_valid, y_valid)\n",
    "    print(f\"Permutation importance for fold {fold_n+1}\")\n",
    "    display(eli5.show_weights(permutation_importance, feature_names = FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n",
    "feature_importances.to_csv('E-Sun_Credit_Card_Fraud_Detection_feature_importances.csv')\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n",
    "plt.title('50 TOP features importance over {} folds average'.format(folds.n_splits));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
