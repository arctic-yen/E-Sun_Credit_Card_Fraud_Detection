{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.56)\n",
    "\n",
    "import gc\n",
    "import os, sys, random, math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, train_test_split\n",
    "from tqdm.autonotebook import tqdm\n",
    "import itertools\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, BatchNormalization, Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from scipy.stats import rankdata, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024 ** 2 # just added \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024 ** 2\n",
    "    percent = 100 * (start_mem - end_mem) / start_mem\n",
    "    print('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_mem, end_mem, percent))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "train = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/train.csv')#, index_col='txkey')\n",
    "print('\\tSuccessfully loaded train!')\n",
    "\n",
    "test = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/test.csv')#, index_col='txkey')\n",
    "print('\\tSuccessfully loaded test!')\n",
    "\n",
    "print('Data was successfully loaded!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_split(dataframe):\n",
    "    dataframe['locdt_M'] = dataframe['locdt'].map({1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:1,10:1,11:1,12:1,13:1,14:1,15:1,\n",
    "                                                   16:1,17:1,18:1,19:1,20:1,21:1,22:1,23:1,24:1,25:1,26:1,27:1,28:1,29:1,30:1,31:1,\n",
    "                                                   32:2,33:2,34:2,35:2,36:2,37:2,38:2,39:2,40:2,41:2,42:2,43:2,44:2,45:2,\n",
    "                                                   46:2,47:2,48:2,49:2,50:2,51:2,52:2,53:2,54:2,55:2,56:2,57:2,58:2,59:2,60:2,61:2,\n",
    "                                                   62:3,63:3,64:3,65:3,66:3,67:3,68:3,69:3,70:3,71:3,72:3,73:3,74:3,75:3,\n",
    "                                                   76:3,77:3,78:3,79:3,80:3,81:3,82:3,83:3,84:3,85:3,86:3,87:3,88:3,89:3,90:3,91:3,92:3,\n",
    "                                                   93:4,94:4,95:4,96:4,97:4,98:4,99:4,100:4,101:4,102:4,103:4,104:4,105:4,106:4,107:4,\n",
    "                                                   108:4,109:4,110:4,111:4,112:4,113:4,114:4,115:4,116:4,117:4,118:4,119:4,120:4})\n",
    "    \n",
    "    dataframe['locdt_W'] = dataframe['locdt'].map({1:1,2:1,3:1,4:1,5:1,6:1,7:1,\n",
    "                                                   8:2,9:2,10:2,11:2,12:2,13:2,14:2,\n",
    "                                                   15:3,16:3,17:3,18:3,19:3,20:3,21:3,\n",
    "                                                   22:4,23:4,24:4,25:4,26:4,27:4,28:4,\n",
    "                                                   29:5,30:5,31:5,32:5,33:5,34:5,35:5,\n",
    "                                                   36:6,37:6,38:6,39:6,40:6,41:6,42:6,\n",
    "                                                   43:7,44:7,45:7,46:7,47:7,48:7,49:7,\n",
    "                                                   50:8,51:8,52:8,53:8,54:8,55:8,56:8,\n",
    "                                                   57:9,58:9,59:9,60:9,61:9,62:9,63:9,\n",
    "                                                   64:10,65:10,66:10,67:10,68:10,69:10,70:10,\n",
    "                                                   71:11,72:11,73:11,74:11,75:11,76:11,77:11,\n",
    "                                                   78:12,79:12,80:12,81:12,82:12,83:12,84:12,\n",
    "                                                   85:13,86:13,87:13,88:13,89:13,90:13,91:13,\n",
    "                                                   92:14,93:14,94:14,95:14,96:14,97:14,98:14,\n",
    "                                                   99:15,100:15,101:15,102:15,103:15,104:15,105:15,\n",
    "                                                   106:16,107:16,108:16,109:16,110:16,111:16,112:16,\n",
    "                                                   113:17,114:17,115:17,116:17,117:17,118:17,119:17,\n",
    "                                                   120:18})\n",
    "    \n",
    "    dataframe['loct_hour'] = (dataframe['loctm']//10000).astype(int)\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "train = date_time_split(train)\n",
    "test = date_time_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['flbmk'] = train['flbmk'].fillna('Nan')\n",
    "train['flg_3dsmk'] = train['flg_3dsmk'].fillna('Nan')\n",
    "\n",
    "test['flbmk'] = test['flbmk'].fillna('Nan')\n",
    "test['flg_3dsmk'] = test['flg_3dsmk'].fillna('Nan')\n",
    "\n",
    "binary_features = [\n",
    "                   'ecfg','flbmk','flg_3dsmk','insfg','ovrlt',\n",
    "                  ]\n",
    "for col in binary_features:\n",
    "    train[col] = train[col].map({'Y':1, 'N':-1, 'Nan':0})\n",
    "    test[col] = test[col].map({'Y':1, 'N':-1, 'Nan':0})\n",
    "\n",
    "train['txn_info'] = train['ecfg'].astype(str)+train['flg_3dsmk'].astype(str)+train['flbmk'].astype(str)\n",
    "test['txn_info'] = test['ecfg'].astype(str)+test['flg_3dsmk'].astype(str)+test['flbmk'].astype(str)\n",
    "\n",
    "cat_columns = ['txn_info']\n",
    "\n",
    "for col in cat_columns:\n",
    "    train[col] = train[col].fillna('unseen_before_label')\n",
    "    test[col]  = test[col].fillna('unseen_before_label')\n",
    "    train[col] = train[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "    \n",
    "    lbl =  LabelEncoder()\n",
    "    lbl.fit(list(train[col]) + list(test[col]))\n",
    "    train[col] = lbl.transform(train[col])\n",
    "    test[col]  = lbl.transform(test[col])\n",
    "\n",
    "train.fillna(-999,inplace = True)\n",
    "test.fillna(-999,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount\n",
    "train['conam_check'] = np.where(train['conam'].isin(test['conam']), 1, -1)\n",
    "test['conam_check']  = np.where(test['conam'].isin(train['conam']), 1, -1)\n",
    "\n",
    "# check cano\n",
    "train['cano_check'] = np.where(train['cano'].isin(test['cano']), 1, -1)\n",
    "test['cano_check']  = np.where(test['cano'].isin(train['cano']), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)\n",
    "temp_train = train.copy()\n",
    "temp_test = test.copy()\n",
    "\n",
    "feq_cols = ['mchno','stocn','csmcu','etymd','stscd','txn_info']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([temp_train[['cano','conam',col]],temp_test[['cano','conam',col]]]) \n",
    "    \n",
    "    new_col_name = 'cano_'+col+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby(['cano',col])['conam'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feq_cols = ['locdt','loct_hour']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([temp_train[['cano','conam',col]],temp_test[['cano','conam',col]]]) \n",
    "    \n",
    "    new_col_name = col+'_cano_count'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby([col,'cano'])['conam'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_all = pd.concat([temp_train[['locdt_W','acqic', 'mchno', 'scity','csmcu','conam']],temp_test[['locdt_W','acqic', 'mchno', 'scity','csmcu','conam']]])\n",
    "train_test_all['past_w_same_store'] = train_test_all.groupby(['locdt_W','acqic', 'mchno','scity','csmcu',])['conam'].transform('count')\n",
    "train['past_w_same_store'] = train_test_all[:train_len].past_w_same_store.tolist()\n",
    "test['past_w_same_store'] = train_test_all[train_len:].past_w_same_store.tolist()\n",
    "\n",
    "train['past_w_same_store'] = np.where(train['past_w_same_store']>=10, 10, train['past_w_same_store'])\n",
    "test['past_w_same_store']  = np.where(test['past_w_same_store']>=10, 10, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = np.where(((train['past_w_same_store']>=5)&(train['past_w_same_store']<10)), 5, train['past_w_same_store'])\n",
    "test['past_w_same_store'] = np.where(((test['past_w_same_store']>=5)&(test['past_w_same_store']<10)), 5, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = np.where(((train['past_w_same_store']>1)&(train['past_w_same_store']<5)), 2, train['past_w_same_store'])\n",
    "test['past_w_same_store'] = np.where(((test['past_w_same_store']>1)&(test['past_w_same_store']<5)), 2, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = train['past_w_same_store'].astype(str)\n",
    "test['past_w_same_store'] = test['past_w_same_store'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feq_cols = ['mchno','stocn','csmcu','etymd','stscd','txn_info']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([temp_train[['cano',col]],temp_test[['cano',col]]]) \n",
    "    \n",
    "    new_col_name = col+'_cano'+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby([col])['cano'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feq_cols = ['mchno','stocn','csmcu']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([temp_train[['locdt','cano',col]],temp_test[['locdt','cano',col]]]) \n",
    "    \n",
    "    new_col_name = 'day_'+col+'_cano'+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby(['locdt',col])['cano'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "\n",
    "            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "\n",
    "            del dt_df['temp_min'],dt_df['temp_max']\n",
    "    return dt_df\n",
    "\n",
    "\n",
    "train_test_all = pd.concat([temp_train[['cano','locdt']],temp_test[['cano','locdt']]])\n",
    "train_test_all.reset_index(inplace=True, drop=True)\n",
    "train_test_all = train_test_all.sort_values('locdt')\n",
    "train_test_all.drop_duplicates('cano',keep='first',inplace=True)\n",
    "\n",
    "train_test_all.set_index('cano',inplace=True)\n",
    "cano_date = train_test_all['locdt'].to_dict()\n",
    "\n",
    "train['first_cano_dt'] = train['cano'].map(cano_date)\n",
    "test['first_cano_dt'] = test['cano'].map(cano_date)\n",
    "\n",
    "train = values_normalization(train, ['locdt_W'], ['first_cano_dt'])\n",
    "test = values_normalization(test, ['locdt_W'], ['first_cano_dt'])\n",
    "\n",
    "del temp_train, temp_test, train_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE\n",
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col],df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype('float32')\n",
    "        print(\"'\"+nm+\"'\",', ',end='')\n",
    "        \n",
    "# LABEL ENCODE\n",
    "def encode_LE(col,train=train,test=test,verbose=True):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "    nm = col\n",
    "    if df_comb.max()>32000: \n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb[len(train):].astype('int16')\n",
    "    del df_comb; x=gc.collect()\n",
    "    if verbose: print(\"'\"+nm+\"'\",', ',end='')\n",
    "\n",
    "# COMBINE FEATURES\n",
    "def encode_CB(col1,col2,df1=train,df2=test):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "    encode_LE(nm,verbose=False)\n",
    "    print(\"'\"+nm+\"'\",', ',end='')\n",
    "\n",
    "# GROUP AGGREGATION MEAN AND STD\n",
    "def encode_AG(main_columns, uids, aggregations=['mean'], train_df=train, test_df=test, \n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-999,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-999,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-999,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "\n",
    "# GROUP AGGREGATION NUNIQUE\n",
    "def encode_AG2(main_columns, uids, train_df=train, test_df=test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('int')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('int')\n",
    "            print(\"'\"+col+'_'+main_column+'_ct'+\"'\",', ',end='')\n",
    "\n",
    "# COUNT ENCODE            \n",
    "def encode_CT(uids,train=train,test=test):\n",
    "    for col in uids:\n",
    "        train[col + '_count_full'] = train[col].map(pd.concat([train[col], test[col]]).value_counts(dropna=False)).astype('int')\n",
    "        test[col + '_count_full'] = test[col].map(pd.concat([train[col], test[col]]).value_counts(dropna=False)).astype('int')\n",
    "        print(\"'\"+col+ '_count_full'+\"'\",', ',end='')\n",
    "\n",
    "def encode_TG(uids,train=train,test=test):\n",
    "    for col in uids:\n",
    "        temp_dict = train.groupby([col])['fraud_ind'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n",
    "        temp_dict.index = temp_dict[col].values\n",
    "        temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "    \n",
    "        train[col+'_target_mean'] = train[col].map(temp_dict)\n",
    "        test[col+'_target_mean']  = test[col].map(temp_dict)\n",
    "        print(\"'\"+col+'_target_mean'+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_AG(['conam'],['cano', 'bacno', 'mcc', 'mchno', 'acqic',],['mean','std'],usena=True)\n",
    "\n",
    "encode_FE(train,test,['cano', 'bacno', 'mcc', 'mchno', 'acqic',])\n",
    "\n",
    "encode_FE(train,test,['etymd', 'stscd', 'txn_info', 'contp', 'hcefg', 'iterm',])\n",
    "\n",
    "encode_TG(['etymd', 'stscd', 'txn_info'],train,test)\n",
    "\n",
    "encode_CB('mcc','mchno')\n",
    "encode_CB('scity','stocn')\n",
    "encode_CB('mchno','stocn')\n",
    "#encode_FE(train,test,['mcc_mchno','scity_stocn','mchno_stocn'])\n",
    "encode_CT(['mcc_mchno','scity_stocn','mchno_stocn'],train,test,)\n",
    "\n",
    "encode_CB('csmcu','stscd')\n",
    "encode_CB('acqic','etymd')\n",
    "encode_FE(train,test,['acqic_etymd','csmcu_stscd'])\n",
    "#encode_CT(['acqic_etymd','csmcu_stscd'],train,test,)\n",
    "\n",
    "encode_CB('stscd','etymd')\n",
    "encode_CB('txn_info','etymd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['uid'] = train['cano'].astype(str)+'_'+train['bacno'].astype(str)\n",
    "test['uid'] = test['cano'].astype(str)+'_'+test['bacno'].astype(str)\n",
    "\n",
    "train['uid2'] = train['uid'].astype(str)+'_'+train['mchno'].astype(str)\n",
    "test['uid2'] = test['uid'].astype(str)+'_'+test['mchno'].astype(str)\n",
    "\n",
    "encode_FE(train,test,['uid'])\n",
    "encode_AG(['conam'],['uid'],['mean','std'],usena=True)\n",
    "\n",
    "encode_AG(['mcc_mchno','scity_stocn','mchno_stocn'],['uid'],['mean','std'],usena=True)\n",
    "encode_AG(['etymd','stscd','txn_info'],['uid'],['mean'],fillna=True,usena=True)\n",
    "encode_AG(['stscd_etymd','txn_info_etymd'],['uid'],['std'],fillna=True,usena=True)\n",
    "encode_AG2(['conam', 'scity_stocn', 'mcc_mchno', 'mchno_stocn'], ['uid'], train_df=train, test_df=test)\n",
    "\n",
    "encode_FE(train,test,['uid2'])\n",
    "encode_AG(['conam'],['uid2'],['mean','std'],usena=True)\n",
    "\n",
    "encode_AG(['acqic_etymd','csmcu_stscd'],['uid2'],['mean','std'],usena=True)\n",
    "encode_AG2(['acqic_etymd', 'csmcu_stscd'], ['uid2'], train_df=train, test_df=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA, FastICA, SparsePCA, KernelPCA, TruncatedSVD\n",
    "\n",
    "cat_cols = ['cano', 'bacno', 'mcc', 'mchno', 'acqic', 'stocn', 'scity', 'csmcu',\n",
    "            'etymd', 'stscd', 'txn_info', 'contp', 'hcefg', 'iterm', 'ovrlt']\n",
    "n_comp = 5\n",
    "\n",
    "temp_train = train[cat_cols].copy()\n",
    "temp_test = test[cat_cols].copy()\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica_results_train = ica.fit_transform(temp_train)\n",
    "ica_results_test = ica.transform(temp_test)\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "tsvd_results_train = tsvd.fit_transform(temp_train)\n",
    "tsvd_results_test = tsvd.transform(temp_test)\n",
    "\n",
    "# SparsePCA\n",
    "spca = SparsePCA(n_components=n_comp, random_state=42)\n",
    "spca_results_train = spca.fit_transform(temp_train)\n",
    "spca_results_test = spca.transform(temp_test)\n",
    "\n",
    "for i in range(1, n_comp + 1):\n",
    "\n",
    "    train['tSVD_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tSVD_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica_results_test[:, i - 1]\n",
    "    \n",
    "    train['spca_' + str(i)] = spca_results_train[:, i - 1]\n",
    "    test['spca_' + str(i)] = spca_results_test[:, i - 1]\n",
    "\n",
    "del temp_train, temp_test, ica_results_train, ica_results_test, tsvd_results_train, tsvd_results_test, spca_results_train, spca_results_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df = reduce_mem_usage(df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "DAE_new_df = pd.read_csv('DAE-hidden-features_new.csv')\n",
    "print('\\tSuccessfully loaded DAE!')\n",
    "\n",
    "for df in [DAE_new_df]:\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "DAE_new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dae = DAE_new_df[:train_len]\n",
    "test_dae = DAE_new_df[train_len:]\n",
    "del DAE_new_df, train_dae['fraud_ind'], test_dae['fraud_ind'], train_dae['Unnamed: 0'], test_dae['Unnamed: 0']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.reset_index(inplace=True)\n",
    "#test.reset_index(inplace=True)\n",
    "train = pd.merge(train, train_dae, on = 'txkey', how = 'left')\n",
    "test = pd.merge(test, test_dae, on = 'txkey', how = 'left')\n",
    "#train.set_index('txkey',drop=True,inplace=True)\n",
    "#test.set_index('txkey',drop=True,inplace=True)\n",
    "del train_dae, test_dae\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape : \"+str(train.shape))\n",
    "print(\"Test shape  : \"+str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null_train = train.isnull().sum()\n",
    "check_null_train[check_null_train>0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null_test = test.isnull().sum()\n",
    "check_null_test[check_null_test>0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features = ['loctm', 'locdt', 'insfg', 'uid', 'uid2',\n",
    "                   'first_cano_dt', 'first_cano_dt_locdt_W_min_max', \n",
    "                   'locdt_W', 'locdt_M', 'txkey'\n",
    "                  ]\n",
    "\n",
    "features_columns = [col for col in list(train) if col not in remove_features]\n",
    "features_columns.remove('fraud_ind')\n",
    "target = 'fraud_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NOW USING THE FOLLOWING',len(features_columns),'FEATURES.')\n",
    "np.array(features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in features_columns:\n",
    "    if train[col].dtype=='O':\n",
    "        print(col)\n",
    "        train[col] = train[col].fillna('unseen_before_label')\n",
    "        test[col]  = test[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train[col] = train[col].astype(str)\n",
    "        test[col] = test[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col])+list(test[col]))\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col]  = le.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[target]\n",
    "X = train[features_columns]\n",
    "X_index = train['txkey']\n",
    "\n",
    "X_test = test[features_columns]\n",
    "\n",
    "print (\"Size of X data : {}\" .format(X.shape))\n",
    "print (\"Size of X_test data : {}\" .format(X_test.shape))\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "                        'cano', 'bacno', 'acqic', 'mchno', 'mcc', 'stocn', 'scity', 'csmcu',\n",
    "                        'contp', 'etymd', 'hcefg', 'iterm', 'stscd', 'txn_info',\n",
    "                        'ecfg', 'flbmk', 'flg_3dsmk', 'ovrlt',\n",
    "                        'conam_check', 'cano_check',\n",
    "                        'mcc_mchno', 'scity_stocn', 'mchno_stocn',\n",
    "                        'csmcu_stscd', 'acqic_etymd', 'stscd_etymd', 'txn_info_etymd',\n",
    "                        'past_w_same_store'\n",
    "                       ]\n",
    "\n",
    "continuous_features = list(filter(lambda x: x not in categorical_features, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in continuous_features:\n",
    "    scaler = StandardScaler()\n",
    "    #if X[column].max() > 100 and X[column].min() >= 0:\n",
    "    #    X[column] = np.log1p(X[column])\n",
    "    #    X_test[column] = np.log1p(X_test[column])\n",
    "    scaler.fit(np.concatenate([X[column].values.reshape(-1,1), X_test[column].values.reshape(-1,1)]))\n",
    "    X[column] = scaler.transform(X[column].values.reshape(-1,1))\n",
    "    X_test[column] = scaler.transform(X_test[column].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize = False,\n",
    "                          title = 'Confusion matrix\"',\n",
    "                          cmap = plt.cm.Blues) :\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment = 'center',\n",
    "                 color = 'white' if cm[i, j] > thresh else 'black')\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class roc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        #print(y_pred_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        \n",
    "        F1_score = f1_score(self.y_val.values, y_pred_val.round())\n",
    "        recalls = recall_score(self.y_val.values, y_pred_val.round())\n",
    "        precisions = precision_score(self.y_val.values , y_pred_val.round())\n",
    "        \n",
    "        print('\\rROC        : %s' % (str(round(roc_val,6))),end=100*' '+'\\n')\n",
    "        print('\\rF1_score   : %s' % (str(round(F1_score,6))),end=100*' '+'\\n')\n",
    "        print('\\rRecall     : %s' % (str(round(recalls,6))),end=100*' '+'\\n')\n",
    "        print('\\rPrecisions : %s' % (str(round(precisions,6))),end=100*' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "    \n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "get_custom_objects().update({'focal_loss_fn': focal_loss()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_dataset(df):\n",
    "    X = {str(col) : np.array(df[col]) for col in df.columns}\n",
    "    return X\n",
    "\n",
    "def create_model(loss_fn):\n",
    "    print('Create Model')\n",
    "    \n",
    "    cat_inputs = []\n",
    "    num_inputs = []\n",
    "    bin_inputs = []\n",
    "    embeddings = []\n",
    "    embedding_layer_names = []\n",
    "    emb_n = 5\n",
    "    \n",
    "    binary_features = ['ecfg', 'flbmk', 'flg_3dsmk', 'ovrlt', 'conam_check', 'cano_check',]\n",
    "    for col in binary_features:\n",
    "        categorical_features.remove(col)\n",
    "\n",
    "    for col in categorical_features:\n",
    "        _input = layers.Input(shape=(1,), name=col)\n",
    "        vocab_size = X[col].nunique()\n",
    "        emb_n = min(10, vocab_size//2+1)\n",
    "        _embed = layers.Embedding(vocab_size, emb_n, name=col+'_emb')(_input)\n",
    "        _embed = layers.Reshape(target_shape=(emb_n,))(_embed)\n",
    "        cat_inputs.append(_input)\n",
    "        embeddings.append(_embed)\n",
    "        embedding_layer_names.append(col+'_emb')\n",
    "    \n",
    "    for col in continuous_features:\n",
    "        numeric_input = layers.Input(shape=(1,), name=col)\n",
    "        num_inputs.append(numeric_input)\n",
    "    \n",
    "    for col in binary_features:\n",
    "        binary_input = layers.Input(shape=(1,), name=col)\n",
    "        bin_inputs.append(binary_input)\n",
    "\n",
    "    merged_num_inputs = layers.concatenate(num_inputs)\n",
    "    merged_bin_inputs = layers.concatenate(bin_inputs)\n",
    "    merged_inputs = layers.concatenate(embeddings)\n",
    "    inps = layers.concatenate([merged_inputs, merged_num_inputs, merged_bin_inputs])\n",
    "    \n",
    "    x = Dense(256, activation='relu')(inps)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=cat_inputs + num_inputs + bin_inputs, outputs=output)\n",
    "    model.compile(\n",
    "          optimizer=Adam(), #Nadam()\n",
    "          loss=[loss_fn],\n",
    "          metrics=['accuracy'],\n",
    "    )\n",
    "    print('Done!')\n",
    "\n",
    "    #print('\\n Model Summary: ')\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "n_epochs = 15\n",
    "patience = 3\n",
    "n_batch_size = 512\n",
    "\n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X.shape[0])\n",
    "\n",
    "NFOLDS = 5\n",
    "folds = KFold(n_splits=NFOLDS)\n",
    "splits = folds.split(X, y)\n",
    "\n",
    "auc_score = 0\n",
    "F1_score = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "cms= []\n",
    "\n",
    "for fold_n, (train_idx, valid_idx) in enumerate(splits):    \n",
    "\n",
    "    x_train_fold= X[features_columns].iloc[train_idx,:]\n",
    "    y_train_fold= y.iloc[train_idx]\n",
    "    x_val_fold= X[features_columns].iloc[valid_idx,:]\n",
    "    y_val_fold= y.iloc[valid_idx]\n",
    "    \n",
    "    model = create_model('focal_loss_fn') #binary_crossentropy\n",
    "    callbacks = [\n",
    "        roc_callback(training_data=(get_keras_dataset(x_train_fold), y_train_fold),\n",
    "                     validation_data=(get_keras_dataset(x_val_fold), y_val_fold)),\n",
    "        EarlyStopping(monitor='val_acc', patience=patience, mode='max', verbose=1),\n",
    "        #ModelCheckpoint(\"model_\" + str(fold_n+1) + \".hdf5\",\n",
    "        #                 save_best_only=True, verbose=1,\n",
    "        #                 monitor='val_acc', mode='max')\n",
    "    ]\n",
    "    model.fit(get_keras_dataset(x_train_fold),\n",
    "              y_train_fold,\n",
    "              epochs=n_epochs,\n",
    "              batch_size=n_batch_size,\n",
    "              callbacks=callbacks,\n",
    "              verbose=1)\n",
    "    #del model\n",
    "    #model = load_model(\"model_\" + str(fold_n+1) + \".hdf5\", custom_objects={'focal_loss_fn': focal_loss()})\n",
    "    y_pred_valid = model.predict(get_keras_dataset(x_val_fold))\n",
    "    y_oof[valid_idx] = y_pred_valid.reshape(y_pred_valid.shape[0])\n",
    "    \n",
    "    \n",
    "    print(f\"\\nFold {fold_n + 1} | AUC: {roc_auc_score(y_val_fold, y_pred_valid)}\")\n",
    "    print(f\"Fold {fold_n + 1} | F1:  {f1_score(y_val_fold.values, y_pred_valid.round())}\")\n",
    "    \n",
    "    auc_score += roc_auc_score(y_val_fold, y_pred_valid) / NFOLDS\n",
    "    F1_score.append(f1_score(y_val_fold.values, y_pred_valid.round())) \n",
    "    recalls.append(recall_score(y_val_fold.values, y_pred_valid.round()))\n",
    "    precisions.append(precision_score(y_val_fold.values , y_pred_valid.round()))\n",
    "    cms.append(confusion_matrix(y_val_fold.values, y_pred_valid.round()))\n",
    "    \n",
    "    \n",
    "    y_temp_preds = model.predict(get_keras_dataset(X_test))\n",
    "    y_preds += y_temp_preds.reshape(y_temp_preds.shape[0]) / NFOLDS\n",
    "\n",
    "print(f\"\\nMean AUC = {auc_score}\")\n",
    "print(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")\n",
    "print(f\"\\nMean F1 Score = {np.mean(F1_score)}\")\n",
    "print(f\"Mean Recall score = {np.mean(recalls)}\")\n",
    "print(f\"Mean Precision score = {np.mean(precisions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion maxtrix & metrics\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "cm = np.average(cms, axis=0)\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, \n",
    "                      classes=class_names, \n",
    "                      title= 'Confusion matrix [averaged/folds]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_oof,bins=100)\n",
    "plt.ylim((0,5000))\n",
    "plt.title('NN OOF')\n",
    "plt.show()\n",
    "\n",
    "X['oof'] = y_oof\n",
    "X['txkey'] = X_index\n",
    "X.reset_index(inplace=True)\n",
    "X[['txkey','oof']].to_csv('oof_NN.csv')\n",
    "X.set_index('txkey',drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_preds,bins=100)\n",
    "plt.ylim((0,5000))\n",
    "plt.title('NN Submission')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize f1 score\n",
    "thresholds = []\n",
    "for thresh in np.arange(0.1, 0.5, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = f1_score(y, (y_oof > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_test = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/submission_test.csv')\n",
    "print('\\tSuccessfully loaded submission_test!')\n",
    "\n",
    "submission_test['fraud_ind'] = y_preds\n",
    "submission_test.to_csv(\"E-Sun_Credit_Card_Fraud_Detection_NN_without_optimize.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
