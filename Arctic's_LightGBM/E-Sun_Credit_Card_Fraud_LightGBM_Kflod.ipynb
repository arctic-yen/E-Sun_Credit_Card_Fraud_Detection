{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zsn_gkrUDcN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.56)\n",
    "import gc\n",
    "\n",
    "import os, sys, random, math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score, confusion_matrix\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6zqrCBOUDcR"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHSNRunAUDcT",
    "outputId": "6395c102-7c23-4f79-c34f-d0bbda0e8c31"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "train = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/train.csv', index_col='txkey')\n",
    "print('\\tSuccessfully loaded train!')\n",
    "\n",
    "test = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/test.csv', index_col='txkey')\n",
    "print('\\tSuccessfully loaded test!')\n",
    "\n",
    "print('Data was successfully loaded!\\n')\n",
    "\n",
    "print(\"Train shape : \"+str(train.shape))\n",
    "print(\"Test shape  : \"+str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_split(dataframe):\n",
    "    dataframe['locdt_M'] = dataframe['locdt'].map({1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:1,10:1,11:1,12:1,13:1,14:1,15:1,\n",
    "                                                   16:1,17:1,18:1,19:1,20:1,21:1,22:1,23:1,24:1,25:1,26:1,27:1,28:1,29:1,30:1,31:1,\n",
    "                                                   32:2,33:2,34:2,35:2,36:2,37:2,38:2,39:2,40:2,41:2,42:2,43:2,44:2,45:2,\n",
    "                                                   46:2,47:2,48:2,49:2,50:2,51:2,52:2,53:2,54:2,55:2,56:2,57:2,58:2,59:2,60:2,61:2,\n",
    "                                                   62:3,63:3,64:3,65:3,66:3,67:3,68:3,69:3,70:3,71:3,72:3,73:3,74:3,75:3,\n",
    "                                                   76:3,77:3,78:3,79:3,80:3,81:3,82:3,83:3,84:3,85:3,86:3,87:3,88:3,89:3,90:3,91:3,92:3,\n",
    "                                                   93:4,94:4,95:4,96:4,97:4,98:4,99:4,100:4,101:4,102:4,103:4,104:4,105:4,106:4,107:4,\n",
    "                                                   108:4,109:4,110:4,111:4,112:4,113:4,114:4,115:4,116:4,117:4,118:4,119:4,120:4})\n",
    "    \n",
    "    dataframe['locdt_W'] = dataframe['locdt'].map({1:1,2:1,3:1,4:1,5:1,6:1,7:1,\n",
    "                                                   8:2,9:2,10:2,11:2,12:2,13:2,14:2,\n",
    "                                                   15:3,16:3,17:3,18:3,19:3,20:3,21:3,\n",
    "                                                   22:4,23:4,24:4,25:4,26:4,27:4,28:4,\n",
    "                                                   29:5,30:5,31:5,32:5,33:5,34:5,35:5,\n",
    "                                                   36:6,37:6,38:6,39:6,40:6,41:6,42:6,\n",
    "                                                   43:7,44:7,45:7,46:7,47:7,48:7,49:7,\n",
    "                                                   50:8,51:8,52:8,53:8,54:8,55:8,56:8,\n",
    "                                                   57:9,58:9,59:9,60:9,61:9,62:9,63:9,\n",
    "                                                   64:10,65:10,66:10,67:10,68:10,69:10,70:10,\n",
    "                                                   71:11,72:11,73:11,74:11,75:11,76:11,77:11,\n",
    "                                                   78:12,79:12,80:12,81:12,82:12,83:12,84:12,\n",
    "                                                   85:13,86:13,87:13,88:13,89:13,90:13,91:13,\n",
    "                                                   92:14,93:14,94:14,95:14,96:14,97:14,98:14,\n",
    "                                                   99:15,100:15,101:15,102:15,103:15,104:15,105:15,\n",
    "                                                   106:16,107:16,108:16,109:16,110:16,111:16,112:16,\n",
    "                                                   113:17,114:17,115:17,116:17,117:17,118:17,119:17,\n",
    "                                                   120:18})\n",
    "    \n",
    "    dataframe['loct_hour'] = (dataframe['loctm']//10000).astype(int)\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "train = date_time_split(train)\n",
    "test = date_time_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['txn_info'] = train['ecfg'].astype(str)+train['flg_3dsmk'].astype(str)+train['flbmk'].astype(str)\n",
    "test['txn_info'] = test['ecfg'].astype(str)+test['flg_3dsmk'].astype(str)+test['flbmk'].astype(str)\n",
    "\n",
    "cat_columns = ['ecfg','flbmk','flg_3dsmk','insfg','ovrlt','txn_info']\n",
    "\n",
    "for col in cat_columns:\n",
    "    train[col] = train[col].fillna('unseen_before_label')\n",
    "    test[col]  = test[col].fillna('unseen_before_label')\n",
    "    train[col] = train[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "    \n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train[col]) + list(test[col]))\n",
    "    train[col] = lbl.transform(train[col])\n",
    "    test[col]  = lbl.transform(test[col])\n",
    "\n",
    "train.fillna(-999,inplace = True)\n",
    "test.fillna(-999,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount\n",
    "train['conam_check'] = np.where(train['conam'].isin(test['conam']), 1, 0)\n",
    "test['conam_check']  = np.where(test['conam'].isin(train['conam']), 1, 0)\n",
    "\n",
    "# check cano\n",
    "train['cano_check'] = np.where(train['cano'].isin(test['cano']), 1, 0)\n",
    "test['cano_check']  = np.where(test['cano'].isin(train['cano']), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)\n",
    "temp_train = train.copy()\n",
    "temp_test = test.copy()\n",
    "\n",
    "feq_cols = ['mchno','stocn','csmcu','etymd','stscd','txn_info',]\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([temp_train[['cano','conam',col]],temp_test[['cano','conam',col]]]) \n",
    "    \n",
    "    new_col_name = 'cano_'+col+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby(['cano',col])['conam'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feq_cols = ['locdt','loct_hour']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([temp_train[['cano','conam',col]],temp_test[['cano','conam',col]]]) \n",
    "    \n",
    "    new_col_name = col+'_cano_count'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby([col,'cano'])['conam'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feq_cols = ['mchno','stocn','csmcu','etymd','stscd','txn_info']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([train[['cano',col]],test[['cano',col]]]) \n",
    "    \n",
    "    new_col_name = col+'_cano'+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby([col])['cano'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feq_cols = ['mchno','stocn','csmcu']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([train[['locdt','cano',col]],test[['locdt','cano',col]]]) \n",
    "    \n",
    "    new_col_name = 'day_'+col+'_cano'+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby(['locdt',col])['cano'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_all = pd.concat([temp_train[['locdt_W','acqic', 'mchno', 'scity','csmcu','conam']],temp_test[['locdt_W','acqic', 'mchno', 'scity','csmcu','conam']]])\n",
    "train_test_all['past_w_same_store'] = train_test_all.groupby(['locdt_W','acqic', 'mchno','scity','csmcu',])['conam'].transform('count')\n",
    "train['past_w_same_store'] = train_test_all[:train_len].past_w_same_store.tolist()\n",
    "test['past_w_same_store'] = train_test_all[train_len:].past_w_same_store.tolist()\n",
    "\n",
    "train['past_w_same_store'] = np.where(train['past_w_same_store']>=10, 10, train['past_w_same_store'])\n",
    "test['past_w_same_store']  = np.where(test['past_w_same_store']>=10, 10, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = np.where(((train['past_w_same_store']>=5)&(train['past_w_same_store']<10)), 5, train['past_w_same_store'])\n",
    "test['past_w_same_store'] = np.where(((test['past_w_same_store']>=5)&(test['past_w_same_store']<10)), 5, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = np.where(((train['past_w_same_store']>1)&(train['past_w_same_store']<5)), 2, train['past_w_same_store'])\n",
    "test['past_w_same_store'] = np.where(((test['past_w_same_store']>1)&(test['past_w_same_store']<5)), 2, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = train['past_w_same_store'].astype(str)\n",
    "test['past_w_same_store'] = test['past_w_same_store'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "\n",
    "            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "\n",
    "            del dt_df['temp_min'],dt_df['temp_max']\n",
    "    return dt_df\n",
    "\n",
    "\n",
    "train_test_all = pd.concat([temp_train[['cano','locdt']],temp_test[['cano','locdt']]],ignore_index=True,sort=False)\n",
    "train_test_all.reset_index(inplace=True, drop=True)\n",
    "train_test_all = train_test_all.sort_values('locdt')\n",
    "train_test_all.drop_duplicates('cano',keep='first',inplace=True)\n",
    "\n",
    "train_test_all.set_index('cano',inplace=True)\n",
    "cano_date = train_test_all['locdt'].to_dict()\n",
    "\n",
    "train['first_cano_dt'] = train['cano'].map(cano_date)\n",
    "test['first_cano_dt'] = test['cano'].map(cano_date)\n",
    "\n",
    "train = values_normalization(train, ['locdt_W'], ['first_cano_dt'])\n",
    "test = values_normalization(test, ['locdt_W'], ['first_cano_dt'])\n",
    "\n",
    "del temp_train, temp_test, train_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape : \"+str(train.shape))\n",
    "print(\"Test shape  : \"+str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE\n",
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col],df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype('float32')\n",
    "        print(\"'\"+nm+\"'\",', ',end='')\n",
    "        \n",
    "# LABEL ENCODE\n",
    "def encode_LE(col,train=train,test=test,verbose=True):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "    nm = col\n",
    "    if df_comb.max()>32000: \n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb[len(train):].astype('int16')\n",
    "    del df_comb; x=gc.collect()\n",
    "    if verbose: print(\"'\"+nm+\"'\",', ',end='')\n",
    "\n",
    "# COMBINE FEATURES\n",
    "def encode_CB(col1,col2,df1=train,df2=test):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "    encode_LE(nm,verbose=False)\n",
    "    print(\"'\"+nm+\"'\",', ',end='')\n",
    "\n",
    "# GROUP AGGREGATION MEAN AND STD\n",
    "def encode_AG(main_columns, uids, aggregations=['mean'], train_df=train, test_df=test, \n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-999,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-999,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-999,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "\n",
    "# GROUP AGGREGATION NUNIQUE\n",
    "def encode_AG2(main_columns, uids, train_df=train, test_df=test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(\"'\"+col+'_'+main_column+'_ct'+\"'\",', ',end='')\n",
    "\n",
    "# COUNT ENCODE            \n",
    "def encode_CT(uids,train=train,test=test):\n",
    "    for col in uids:\n",
    "        train[col + '_count_full'] = train[col].map(pd.concat([train[col], test[col]]).value_counts(dropna=False))\n",
    "        test[col + '_count_full'] = test[col].map(pd.concat([train[col], test[col]]).value_counts(dropna=False))\n",
    "        print(\"'\"+col+ '_count_full'+\"'\",', ',end='')\n",
    "\n",
    "def encode_TG(uids,train=train,test=test):\n",
    "    for col in uids:\n",
    "        temp_dict = train.groupby([col])['fraud_ind'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n",
    "        temp_dict.index = temp_dict[col].values\n",
    "        temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "    \n",
    "        train[col+'_target_mean'] = train[col].map(temp_dict)\n",
    "        test[col+'_target_mean']  = test[col].map(temp_dict)\n",
    "        print(\"'\"+col+'_target_mean'+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_AG(['conam'],['cano', 'bacno', 'mcc', 'mchno', 'acqic',],['mean','std'],usena=True)\n",
    "\n",
    "encode_FE(train,test,['cano', 'bacno', 'mcc', 'mchno', 'acqic',])\n",
    "\n",
    "encode_FE(train,test,['etymd', 'stscd', 'txn_info', 'contp', 'hcefg', 'iterm',])\n",
    "\n",
    "encode_TG(['etymd', 'stscd', 'txn_info'],train,test)\n",
    "\n",
    "encode_CB('mcc','mchno')\n",
    "encode_CB('scity','stocn')\n",
    "encode_CB('mchno','stocn')\n",
    "#encode_FE(train,test,['mcc_mchno','scity_stocn','mchno_stocn'])\n",
    "encode_CT(['mcc_mchno','scity_stocn','mchno_stocn'],train,test,)\n",
    "\n",
    "encode_CB('csmcu','stscd')\n",
    "encode_CB('acqic','etymd')\n",
    "encode_FE(train,test,['acqic_etymd','csmcu_stscd'])\n",
    "#encode_CT(['acqic_etymd','csmcu_stscd'],train,test,)\n",
    "\n",
    "encode_CB('stscd','etymd')\n",
    "encode_CB('txn_info','etymd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['uid'] = train['cano'].astype(str)+'_'+train['bacno'].astype(str)\n",
    "test['uid'] = test['cano'].astype(str)+'_'+test['bacno'].astype(str)\n",
    "\n",
    "train['uid2'] = train['uid'].astype(str)+'_'+train['mchno'].astype(str)\n",
    "test['uid2'] = test['uid'].astype(str)+'_'+test['mchno'].astype(str)\n",
    "\n",
    "encode_FE(train,test,['uid'])\n",
    "encode_AG(['conam'],['uid'],['mean','std'],usena=True)\n",
    "\n",
    "encode_AG(['mcc_mchno','scity_stocn','mchno_stocn'],['uid'],['mean','std'],usena=True)\n",
    "encode_AG(['etymd','stscd','txn_info'],['uid'],['mean'],fillna=True,usena=True)\n",
    "\n",
    "encode_AG2(['conam', 'scity_stocn', 'mcc_mchno', 'mchno_stocn'], ['uid'], train_df=train, test_df=test)\n",
    "\n",
    "encode_FE(train,test,['uid2'])\n",
    "encode_AG(['conam'],['uid2'],['mean','std'],usena=True)\n",
    "\n",
    "encode_AG(['acqic_etymd','csmcu_stscd'],['uid2'],['mean','std'],usena=True)\n",
    "encode_AG(['stscd_etymd','txn_info_etymd'],['uid'],['std'],fillna=True,usena=True)\n",
    "encode_AG2(['acqic_etymd', 'csmcu_stscd'], ['uid'], train_df=train, test_df=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['conam'] = np.log(train['conam'])\n",
    "test['conam'] = np.log(test['conam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_6Kpu3wUDc0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import FastICA, SparsePCA, TruncatedSVD\n",
    "\n",
    "cat_cols = ['cano', 'bacno', 'mcc', 'mchno', 'acqic', 'stocn', 'scity', 'csmcu', \n",
    "            'etymd', 'stscd', 'txn_info', 'contp', 'hcefg', 'iterm', 'ovrlt']\n",
    "n_comp = 5\n",
    "\n",
    "temp_train = train[cat_cols].copy()\n",
    "temp_test = test[cat_cols].copy()\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica_results_train = ica.fit_transform(temp_train)\n",
    "ica_results_test = ica.transform(temp_test)\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "tsvd_results_train = tsvd.fit_transform(temp_train)\n",
    "tsvd_results_test = tsvd.transform(temp_test)\n",
    "\n",
    "# SparsePCA\n",
    "spca = SparsePCA(n_components=n_comp, random_state=42)\n",
    "spca_results_train = spca.fit_transform(temp_train)\n",
    "spca_results_test = spca.transform(temp_test)\n",
    "\n",
    "for i in range(1, n_comp + 1):\n",
    "\n",
    "    train['tSVD_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tSVD_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica_results_test[:, i - 1]\n",
    "    \n",
    "    train['spca_' + str(i)] = ica_results_train[:, i - 1]\n",
    "    test['spca_' + str(i)] = ica_results_test[:, i - 1]\n",
    "\n",
    "del temp_train, temp_test, ica_results_train, ica_results_test, tsvd_results_train, tsvd_results_test, spca_results_train, spca_results_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaEncoder(object):\n",
    "        \n",
    "    def __init__(self, group):\n",
    "        \n",
    "        self.group = group\n",
    "        self.stats = None\n",
    "        \n",
    "    # get counts from df\n",
    "    def fit(self, df, target_col):\n",
    "        self.prior_mean = np.mean(df[target_col])\n",
    "        stats = df[[target_col, self.group]].groupby(self.group)\n",
    "        stats = stats.agg(['sum', 'count'])[target_col]    \n",
    "        stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n",
    "        stats.reset_index(level=0, inplace=True)           \n",
    "        self.stats = stats\n",
    "        \n",
    "    # extract posterior statistics\n",
    "    def transform(self, df, stat_type, N_min=1):\n",
    "        \n",
    "        df_stats = pd.merge(df[[self.group]], self.stats, how='left')\n",
    "        n = df_stats['n'].copy()\n",
    "        N = df_stats['N'].copy()\n",
    "        \n",
    "        # fill in missing\n",
    "        nan_indexs = np.isnan(n)\n",
    "        n[nan_indexs] = self.prior_mean\n",
    "        N[nan_indexs] = 1.0\n",
    "        \n",
    "        # prior parameters\n",
    "        N_prior = np.maximum(N_min-N, 0)\n",
    "        alpha_prior = self.prior_mean*N_prior\n",
    "        beta_prior = (1-self.prior_mean)*N_prior\n",
    "        \n",
    "        # posterior parameters\n",
    "        alpha = alpha_prior + n\n",
    "        beta =  beta_prior + N-n\n",
    "        \n",
    "        # calculate statistics\n",
    "        if stat_type=='mean':\n",
    "            num = alpha\n",
    "            dem = alpha+beta\n",
    "                    \n",
    "        elif stat_type=='mode':\n",
    "            num = alpha-1\n",
    "            dem = alpha+beta-2\n",
    "            \n",
    "        elif stat_type=='median':\n",
    "            num = alpha-1/3\n",
    "            dem = alpha+beta-2/3\n",
    "        \n",
    "        elif stat_type=='var':\n",
    "            num = alpha*beta\n",
    "            dem = (alpha+beta)**2*(alpha+beta+1)\n",
    "                    \n",
    "        elif stat_type=='skewness':\n",
    "            num = 2*(beta-alpha)*np.sqrt(alpha+beta+1)\n",
    "            dem = (alpha+beta+2)*np.sqrt(alpha*beta)\n",
    "\n",
    "        elif stat_type=='kurtosis':\n",
    "            num = 6*(alpha-beta)**2*(alpha+beta+1) - alpha*beta*(alpha+beta+2)\n",
    "            dem = alpha*beta*(alpha+beta+2)*(alpha+beta+3)\n",
    "            \n",
    "        # replace missing\n",
    "        value = num/dem\n",
    "        value[np.isnan(value)] = np.nanmedian(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['mchno','scity','acqic']\n",
    "\n",
    "#N_min = 1000  \n",
    "# encode variables\n",
    "for col in cat_columns:\n",
    "    all_data = pd.concat([train[col],test[col]])\n",
    "    N_min = (len(all_data.unique()))* 0.1\n",
    "    # fit encoder\n",
    "    be = BetaEncoder(col)\n",
    "    be.fit(train, 'fraud_ind')\n",
    "\n",
    "    # mean\n",
    "    train[col] = be.transform(train, 'mean', N_min)\n",
    "    test[col]  = be.transform(test,  'mean', N_min)\n",
    "    \n",
    "    del all_data, N_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape : \"+str(train.shape))\n",
    "print(\"Test shape  : \"+str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzXqBZIKUDcZ",
    "outputId": "76d4c11f-e740-4000-a0f6-891340cc5c35"
   },
   "outputs": [],
   "source": [
    "check_null_train = train.isnull().sum()\n",
    "check_null_train[check_null_train>0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJWcFQT7UDcs",
    "outputId": "5fbe6a99-e76b-4b2a-bdef-d77f627d4330"
   },
   "outputs": [],
   "source": [
    "check_null_test = test.isnull().sum()\n",
    "check_null_test[check_null_test>0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zt0mhsH2UDd7",
    "outputId": "ed80918c-6f6f-4e85-d1f1-3fddbd9aaa59"
   },
   "outputs": [],
   "source": [
    "remove_features = ['loctm', 'locdt', 'uid', 'uid2', 'insfg',\n",
    "                   'first_cano_dt', 'first_cano_dt_locdt_W_min_max', \n",
    "                   'locdt_W', 'locdt_M', #'loct_hour', \n",
    "                  ]\n",
    "\n",
    "features_columns = [col for col in list(train) if col not in remove_features]\n",
    "features_columns.remove('fraud_ind')\n",
    "target = 'fraud_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ow0ZezCtUDd-"
   },
   "outputs": [],
   "source": [
    "for col in features_columns:\n",
    "    if train[col].dtype=='O':\n",
    "        print(col)\n",
    "        train[col] = train[col].fillna('unseen_before_label')\n",
    "        test[col]  = test[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train[col] = train[col].astype(str)\n",
    "        test[col] = test[col].astype(str)\n",
    "        \n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(list(train[col])+list(test[col]))\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col]  = le.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize = False,\n",
    "                          title = 'Confusion matrix\"',\n",
    "                          cmap = plt.cm.Blues) :\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment = 'center',\n",
    "                 color = 'white' if cm[i, j] > thresh else 'black')\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RApkoDtbUDeA"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LE2CdFIMUDeR",
    "outputId": "c4ad8901-ad7d-41b8-8d26-51f21d0e7e0e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NFOLDS = 5\n",
    "folds = KFold(n_splits=NFOLDS)\n",
    "splits = folds.split(train, train.fraud_ind.values)\n",
    "\n",
    "#NFOLDS = 3\n",
    "#folds = GroupKFold(n_splits=NFOLDS)\n",
    "#split_groups = train['locdt_M']\n",
    "#splits = folds.split(train, train.fraud_ind.values, groups=split_groups)\n",
    "\n",
    "oof = np.zeros(len(train))\n",
    "\n",
    "roc_aucs = []\n",
    "F1_score = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "cms = []\n",
    "\n",
    "predictions = np.zeros(len(test))\n",
    "predictions_vote = np.zeros(len(test))\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    print(\"\\nfold_ {}\".format(fold_n + 1))\n",
    "    \n",
    "    X_train= train[features_columns].iloc[train_index,:]\n",
    "    y_train= train[target].iloc[train_index]\n",
    "    X_test= train[features_columns].iloc[valid_index,:]\n",
    "    y_test= train[target].iloc[valid_index]\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(n_estimators = 100000,\n",
    "                             objective = 'binary',\n",
    "                             random_state = SEED,\n",
    "                             subsample = 0.7,\n",
    "                             colsample_bytree = 0.7,\n",
    "                             learning_rate = 0.005,\n",
    "                             importance_type = 'gain',\n",
    "                             max_depth = -1,\n",
    "                             num_leaves = 256,\n",
    "                             min_child_samples = 20,\n",
    "                             min_split_gain = 0.001,\n",
    "                             bagging_freq=1,\n",
    "                             reg_alpha = 0,\n",
    "                             reg_lambda = 0,\n",
    "                             n_jobs = -1,\n",
    "                             device='gpu',\n",
    "                             metric='None')\n",
    "    \n",
    "    clf.fit(X_train,y_train,\n",
    "            eval_set = [(X_train,y_train),(X_test,y_test)],\n",
    "            eval_metric = 'auc',\n",
    "            early_stopping_rounds=500,\n",
    "            verbose=200)\n",
    "\n",
    "    temp_oof  = clf.predict_proba(X_test)[:,1]\n",
    "    oof[valid_index] = temp_oof\n",
    "\n",
    "    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_test, temp_oof)}\")\n",
    "    \n",
    "    # Optimize f1 score\n",
    "    thresholds = []\n",
    "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        res = f1_score(y_test, (oof[valid_index] > thresh).astype(int))\n",
    "        thresholds.append([thresh, res])\n",
    "        print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "        \n",
    "    thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_thresh = thresholds[0][0]\n",
    "    print(\"Best threshold: \", best_thresh)\n",
    "    \n",
    "    predictions_vote += ((clf.predict_proba(test[features_columns])[:,1]) > best_thresh).astype(int) / NFOLDS\n",
    "    predictions += clf.predict_proba(test[features_columns])[:,1] / NFOLDS\n",
    "    \n",
    "    \n",
    "    # Scores \n",
    "    roc_aucs.append(roc_auc_score(y_test, temp_oof))\n",
    "    F1_score.append(f1_score(y_test.values, temp_oof.round()))\n",
    "    recalls.append(recall_score(y_test.values, temp_oof.round()))\n",
    "    precisions.append(precision_score(y_test.values, temp_oof.round()))\n",
    "    cms.append(confusion_matrix(y_test.values, temp_oof.round()))\n",
    "    \n",
    "    # Features imp\n",
    "    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importances_\n",
    "\n",
    "    gc.collect()\n",
    "# Metrics\n",
    "print(\n",
    "        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n",
    "        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(F1_score), np.std(F1_score)),\n",
    "        '\\nCV recalls score    : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n",
    "        '\\nCV precisions score : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion maxtrix & metrics\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "cm = np.average(cms, axis=0)\n",
    "class_names = [0,1]\n",
    "plt.figure(figsize=(8, 8))\n",
    "plot_confusion_matrix(cm, \n",
    "                      classes=class_names, \n",
    "                      title= 'Confusion matrix [averaged/folds]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize f1 score\n",
    "thresholds = []\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = f1_score(train[target], (oof > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFVEsMBEUDeZ",
    "outputId": "94cdc081-9df0-44d7-bf39-9a909847276c"
   },
   "outputs": [],
   "source": [
    "feature_importances['feature'] = features_columns\n",
    "\n",
    "feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n",
    "feature_importances.to_csv('E-Sun_Credit_Card_Fraud_Detection_feature_importances_Kflod.csv')\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(60), x='average', y='feature');\n",
    "plt.title('60 TOP features importance over {} folds average'.format(folds.n_splits));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(oof,bins=100)\n",
    "plt.ylim((0,5000))\n",
    "plt.title('lightgbm OOF')\n",
    "plt.show()\n",
    "\n",
    "train['oof'] = oof\n",
    "train.reset_index(inplace=True)\n",
    "train[['txkey','oof']].to_csv('oof_lightgbm.csv')\n",
    "train.set_index('txkey',drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions,bins=100)\n",
    "plt.ylim((0,5000))\n",
    "plt.title('lightgbm Submission')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/submission_test.csv')\n",
    "print('\\tSuccessfully loaded sample_submission!')\n",
    "\n",
    "sub['fraud_ind'] = predictions\n",
    "sub.to_csv(\"E-Sun_Credit_Card_Fraud_Detection_lightgbm_without_optimize.csv\", index=False)\n",
    "\n",
    "sub['fraud_ind'] = (predictions_vote > 0.5).astype(int)\n",
    "sub.to_csv(\"E-Sun_Credit_Card_Fraud_Detection_lightgbm_optimize.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "E-Sun_Credit_Card_Fraud_LightGBM_Kflod_0.653392.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
