{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.56)\n",
    "import gc\n",
    "\n",
    "import os, sys, random, math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold, GroupKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score, confusion_matrix\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\tSuccessfully loaded train!\n",
      "\tSuccessfully loaded test!\n",
      "Data was successfully loaded!\n",
      "\n",
      "Train shape : (1521787, 22)\n",
      "Test shape  : (421665, 21)\n",
      "Wall time: 4.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "train = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/train.csv', index_col='txkey')\n",
    "print('\\tSuccessfully loaded train!')\n",
    "\n",
    "test = pd.read_csv('E-Sun_Credit_Card_Fraud_Data/test.csv', index_col='txkey')\n",
    "print('\\tSuccessfully loaded test!')\n",
    "\n",
    "print('Data was successfully loaded!\\n')\n",
    "\n",
    "print(\"Train shape : \"+str(train.shape))\n",
    "print(\"Test shape  : \"+str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_split(dataframe):\n",
    "    dataframe['locdt_M'] = dataframe['locdt'].map({1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:1,10:1,11:1,12:1,13:1,14:1,15:1,\n",
    "                                                   16:1,17:1,18:1,19:1,20:1,21:1,22:1,23:1,24:1,25:1,26:1,27:1,28:1,29:1,30:1,31:1,\n",
    "                                                   32:2,33:2,34:2,35:2,36:2,37:2,38:2,39:2,40:2,41:2,42:2,43:2,44:2,45:2,\n",
    "                                                   46:2,47:2,48:2,49:2,50:2,51:2,52:2,53:2,54:2,55:2,56:2,57:2,58:2,59:2,60:2,61:2,\n",
    "                                                   62:3,63:3,64:3,65:3,66:3,67:3,68:3,69:3,70:3,71:3,72:3,73:3,74:3,75:3,\n",
    "                                                   76:3,77:3,78:3,79:3,80:3,81:3,82:3,83:3,84:3,85:3,86:3,87:3,88:3,89:3,90:3,91:3,92:3,\n",
    "                                                   93:4,94:4,95:4,96:4,97:4,98:4,99:4,100:4,101:4,102:4,103:4,104:4,105:4,106:4,107:4,\n",
    "                                                   108:4,109:4,110:4,111:4,112:4,113:4,114:4,115:4,116:4,117:4,118:4,119:4,120:4})\n",
    "    \n",
    "    dataframe['locdt_W'] = dataframe['locdt'].map({1:1,2:1,3:1,4:1,5:1,6:1,7:1,\n",
    "                                                   8:2,9:2,10:2,11:2,12:2,13:2,14:2,\n",
    "                                                   15:3,16:3,17:3,18:3,19:3,20:3,21:3,\n",
    "                                                   22:4,23:4,24:4,25:4,26:4,27:4,28:4,\n",
    "                                                   29:5,30:5,31:5,32:5,33:5,34:5,35:5,\n",
    "                                                   36:6,37:6,38:6,39:6,40:6,41:6,42:6,\n",
    "                                                   43:7,44:7,45:7,46:7,47:7,48:7,49:7,\n",
    "                                                   50:8,51:8,52:8,53:8,54:8,55:8,56:8,\n",
    "                                                   57:9,58:9,59:9,60:9,61:9,62:9,63:9,\n",
    "                                                   64:10,65:10,66:10,67:10,68:10,69:10,70:10,\n",
    "                                                   71:11,72:11,73:11,74:11,75:11,76:11,77:11,\n",
    "                                                   78:12,79:12,80:12,81:12,82:12,83:12,84:12,\n",
    "                                                   85:13,86:13,87:13,88:13,89:13,90:13,91:13,\n",
    "                                                   92:14,93:14,94:14,95:14,96:14,97:14,98:14,\n",
    "                                                   99:15,100:15,101:15,102:15,103:15,104:15,105:15,\n",
    "                                                   106:16,107:16,108:16,109:16,110:16,111:16,112:16,\n",
    "                                                   113:17,114:17,115:17,116:17,117:17,118:17,119:17,\n",
    "                                                   120:18})\n",
    "    \n",
    "    dataframe['loct_hour'] = (dataframe['loctm']//10000).astype(int)\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "train = date_time_split(train)\n",
    "test = date_time_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['txn_info'] = train['ecfg'].astype(str)+train['flg_3dsmk'].astype(str)+train['flbmk'].astype(str)\n",
    "test['txn_info'] = test['ecfg'].astype(str)+test['flg_3dsmk'].astype(str)+test['flbmk'].astype(str)\n",
    "\n",
    "cat_columns = ['ecfg','flbmk','flg_3dsmk','insfg','ovrlt','txn_info']\n",
    "\n",
    "for col in cat_columns:\n",
    "    train[col] = train[col].fillna('unseen_before_label')\n",
    "    test[col]  = test[col].fillna('unseen_before_label')\n",
    "    train[col] = train[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "    \n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train[col]) + list(test[col]))\n",
    "    train[col] = lbl.transform(train[col])\n",
    "    test[col]  = lbl.transform(test[col])\n",
    "\n",
    "train.fillna(-999,inplace = True)\n",
    "test.fillna(-999,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount\n",
    "train['conam_check'] = np.where(train['conam'].isin(test['conam']), 1, 0)\n",
    "test['conam_check']  = np.where(test['conam'].isin(train['conam']), 1, 0)\n",
    "\n",
    "# check cano\n",
    "train['cano_check'] = np.where(train['cano'].isin(test['cano']), 1, 0)\n",
    "test['cano_check']  = np.where(test['cano'].isin(train['cano']), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cano_mchno_fq' , 'cano_stocn_fq' , 'cano_csmcu_fq' , 'cano_etymd_fq' , 'cano_stscd_fq' , 'cano_txn_info_fq' , "
     ]
    }
   ],
   "source": [
    "train_len = len(train)\n",
    "temp_train = train.copy()\n",
    "temp_test = test.copy()\n",
    "\n",
    "feq_cols = ['mchno','stocn','csmcu','etymd','stscd','txn_info',]\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([temp_train[['cano','conam',col]],temp_test[['cano','conam',col]]]) \n",
    "    \n",
    "    new_col_name = 'cano_'+col+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby(['cano',col])['conam'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'locdt_cano_count' , 'loct_hour_cano_count' , "
     ]
    }
   ],
   "source": [
    "feq_cols = ['locdt','loct_hour']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([temp_train[['cano','conam',col]],temp_test[['cano','conam',col]]]) \n",
    "    \n",
    "    new_col_name = col+'_cano_count'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby([col,'cano'])['conam'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mchno_cano_fq' , 'stocn_cano_fq' , 'csmcu_cano_fq' , 'etymd_cano_fq' , 'stscd_cano_fq' , 'txn_info_cano_fq' , "
     ]
    }
   ],
   "source": [
    "feq_cols = ['mchno','stocn','csmcu','etymd','stscd','txn_info']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([train[['cano',col]],test[['cano',col]]]) \n",
    "    \n",
    "    new_col_name = col+'_cano'+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby([col])['cano'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'day_mchno_cano_fq' , 'day_stocn_cano_fq' , 'day_csmcu_cano_fq' , "
     ]
    }
   ],
   "source": [
    "feq_cols = ['mchno','stocn','csmcu']\n",
    "for col in feq_cols:\n",
    "    train_test_all = pd.concat([train[['locdt','cano',col]],test[['locdt','cano',col]]]) \n",
    "    \n",
    "    new_col_name = 'day_'+col+'_cano'+'_fq'\n",
    "    train_test_all[new_col_name] = train_test_all.groupby(['locdt',col])['cano'].transform('count')\n",
    "    train[new_col_name] = train_test_all[:train_len][new_col_name].tolist()\n",
    "    test[new_col_name] = train_test_all[train_len:][new_col_name].tolist()\n",
    "    \n",
    "    print(\"'\"+new_col_name+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_all = pd.concat([temp_train[['locdt_W','acqic', 'mchno', 'scity','csmcu','conam']],temp_test[['locdt_W','acqic', 'mchno', 'scity','csmcu','conam']]])\n",
    "train_test_all['past_w_same_store'] = train_test_all.groupby(['locdt_W','acqic', 'mchno','scity','csmcu',])['conam'].transform('count')\n",
    "train['past_w_same_store'] = train_test_all[:train_len].past_w_same_store.tolist()\n",
    "test['past_w_same_store'] = train_test_all[train_len:].past_w_same_store.tolist()\n",
    "\n",
    "train['past_w_same_store'] = np.where(train['past_w_same_store']>=10, 10, train['past_w_same_store'])\n",
    "test['past_w_same_store']  = np.where(test['past_w_same_store']>=10, 10, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = np.where(((train['past_w_same_store']>=5)&(train['past_w_same_store']<10)), 5, train['past_w_same_store'])\n",
    "test['past_w_same_store'] = np.where(((test['past_w_same_store']>=5)&(test['past_w_same_store']<10)), 5, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = np.where(((train['past_w_same_store']>1)&(train['past_w_same_store']<5)), 2, train['past_w_same_store'])\n",
    "test['past_w_same_store'] = np.where(((test['past_w_same_store']>1)&(test['past_w_same_store']<5)), 2, test['past_w_same_store'])\n",
    "\n",
    "train['past_w_same_store'] = train['past_w_same_store'].astype(str)\n",
    "test['past_w_same_store'] = test['past_w_same_store'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "\n",
    "            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "\n",
    "            del dt_df['temp_min'],dt_df['temp_max']\n",
    "    return dt_df\n",
    "\n",
    "\n",
    "train_test_all = pd.concat([temp_train[['cano','locdt']],temp_test[['cano','locdt']]],ignore_index=True,sort=False)\n",
    "train_test_all.reset_index(inplace=True, drop=True)\n",
    "train_test_all = train_test_all.sort_values('locdt')\n",
    "train_test_all.drop_duplicates('cano',keep='first',inplace=True)\n",
    "\n",
    "train_test_all.set_index('cano',inplace=True)\n",
    "cano_date = train_test_all['locdt'].to_dict()\n",
    "\n",
    "train['first_cano_dt'] = train['cano'].map(cano_date)\n",
    "test['first_cano_dt'] = test['cano'].map(cano_date)\n",
    "\n",
    "train = values_normalization(train, ['locdt_W'], ['first_cano_dt'])\n",
    "test = values_normalization(test, ['locdt_W'], ['first_cano_dt'])\n",
    "\n",
    "del temp_train, temp_test, train_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE\n",
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col],df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype('float32')\n",
    "        print(\"'\"+nm+\"'\",', ',end='')\n",
    "        \n",
    "# LABEL ENCODE\n",
    "def encode_LE(col,train=train,test=test,verbose=True):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "    nm = col\n",
    "    if df_comb.max()>32000: \n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb[len(train):].astype('int16')\n",
    "    del df_comb; x=gc.collect()\n",
    "    if verbose: print(\"'\"+nm+\"'\",', ',end='')\n",
    "\n",
    "# COMBINE FEATURES\n",
    "def encode_CB(col1,col2,df1=train,df2=test):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "    encode_LE(nm,verbose=False)\n",
    "    print(\"'\"+nm+\"'\",', ',end='')\n",
    "\n",
    "# GROUP AGGREGATION MEAN AND STD\n",
    "def encode_AG(main_columns, uids, aggregations=['mean'], train_df=train, test_df=test, \n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-999,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-999,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-999,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "\n",
    "# GROUP AGGREGATION NUNIQUE\n",
    "def encode_AG2(main_columns, uids, train_df=train, test_df=test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(\"'\"+col+'_'+main_column+'_ct'+\"'\",', ',end='')\n",
    "\n",
    "# COUNT ENCODE            \n",
    "def encode_CT(uids,train=train,test=test):\n",
    "    for col in uids:\n",
    "        train[col + '_count_full'] = train[col].map(pd.concat([train[col], test[col]]).value_counts(dropna=False))\n",
    "        test[col + '_count_full'] = test[col].map(pd.concat([train[col], test[col]]).value_counts(dropna=False))\n",
    "        print(\"'\"+col+ '_count_full'+\"'\",', ',end='')\n",
    "\n",
    "def encode_TG(uids,train=train,test=test):\n",
    "    for col in uids:\n",
    "        temp_dict = train.groupby([col])['fraud_ind'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n",
    "        temp_dict.index = temp_dict[col].values\n",
    "        temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "    \n",
    "        train[col+'_target_mean'] = train[col].map(temp_dict)\n",
    "        test[col+'_target_mean']  = test[col].map(temp_dict)\n",
    "        print(\"'\"+col+'_target_mean'+\"'\",', ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'conam_cano_mean' , 'conam_cano_std' , 'conam_bacno_mean' , 'conam_bacno_std' , 'conam_mcc_mean' , 'conam_mcc_std' , 'conam_mchno_mean' , 'conam_mchno_std' , 'conam_acqic_mean' , 'conam_acqic_std' , 'cano_FE' , 'bacno_FE' , 'mcc_FE' , 'mchno_FE' , 'acqic_FE' , 'etymd_FE' , 'stscd_FE' , 'txn_info_FE' , 'contp_FE' , 'hcefg_FE' , 'iterm_FE' , 'etymd_target_mean' , 'stscd_target_mean' , 'txn_info_target_mean' , 'mcc_mchno' , 'scity_stocn' , 'mchno_stocn' , 'mcc_mchno_count_full' , 'scity_stocn_count_full' , 'mchno_stocn_count_full' , 'csmcu_stscd' , 'acqic_etymd' , 'acqic_etymd_FE' , 'csmcu_stscd_FE' , 'stscd_etymd' , 'txn_info_etymd' , "
     ]
    }
   ],
   "source": [
    "encode_AG(['conam'],['cano', 'bacno', 'mcc', 'mchno', 'acqic',],['mean','std'],usena=True)\n",
    "\n",
    "encode_FE(train,test,['cano', 'bacno', 'mcc', 'mchno', 'acqic',])\n",
    "\n",
    "encode_FE(train,test,['etymd', 'stscd', 'txn_info', 'contp', 'hcefg', 'iterm',])\n",
    "\n",
    "encode_TG(['etymd', 'stscd', 'txn_info'],train,test)\n",
    "\n",
    "encode_CB('mcc','mchno')\n",
    "encode_CB('scity','stocn')\n",
    "encode_CB('mchno','stocn')\n",
    "#encode_FE(train,test,['mcc_mchno','scity_stocn','mchno_stocn'])\n",
    "encode_CT(['mcc_mchno','scity_stocn','mchno_stocn'],train,test,)\n",
    "\n",
    "encode_CB('csmcu','stscd')\n",
    "encode_CB('acqic','etymd')\n",
    "encode_FE(train,test,['acqic_etymd','csmcu_stscd'])\n",
    "#encode_CT(['acqic_etymd','csmcu_stscd'],train,test,)\n",
    "\n",
    "encode_CB('stscd','etymd')\n",
    "encode_CB('txn_info','etymd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'uid_FE' , 'conam_uid_mean' , 'conam_uid_std' , 'mcc_mchno_uid_mean' , 'mcc_mchno_uid_std' , 'scity_stocn_uid_mean' , 'scity_stocn_uid_std' , 'mchno_stocn_uid_mean' , 'mchno_stocn_uid_std' , 'etymd_uid_mean' , 'stscd_uid_mean' , 'txn_info_uid_mean' , 'uid_conam_ct' , 'uid_scity_stocn_ct' , 'uid_mcc_mchno_ct' , 'uid_mchno_stocn_ct' , 'uid2_FE' , 'conam_uid2_mean' , 'conam_uid2_std' , 'acqic_etymd_uid2_mean' , 'acqic_etymd_uid2_std' , 'csmcu_stscd_uid2_mean' , 'csmcu_stscd_uid2_std' , 'stscd_etymd_uid_std' , 'txn_info_etymd_uid_std' , 'uid_acqic_etymd_ct' , 'uid_csmcu_stscd_ct' , "
     ]
    }
   ],
   "source": [
    "train['uid'] = train['cano'].astype(str)+'_'+train['bacno'].astype(str)\n",
    "test['uid'] = test['cano'].astype(str)+'_'+test['bacno'].astype(str)\n",
    "\n",
    "train['uid2'] = train['uid'].astype(str)+'_'+train['mchno'].astype(str)\n",
    "test['uid2'] = test['uid'].astype(str)+'_'+test['mchno'].astype(str)\n",
    "\n",
    "encode_FE(train,test,['uid'])\n",
    "encode_AG(['conam'],['uid'],['mean','std'],usena=True)\n",
    "\n",
    "encode_AG(['mcc_mchno','scity_stocn','mchno_stocn'],['uid'],['mean','std'],usena=True)\n",
    "encode_AG(['etymd','stscd','txn_info'],['uid'],['mean'],fillna=True,usena=True)\n",
    "\n",
    "encode_AG2(['conam', 'scity_stocn', 'mcc_mchno', 'mchno_stocn'], ['uid'], train_df=train, test_df=test)\n",
    "\n",
    "encode_FE(train,test,['uid2'])\n",
    "encode_AG(['conam'],['uid2'],['mean','std'],usena=True)\n",
    "\n",
    "encode_AG(['acqic_etymd','csmcu_stscd'],['uid2'],['mean','std'],usena=True)\n",
    "encode_AG(['stscd_etymd','txn_info_etymd'],['uid'],['std'],fillna=True,usena=True)\n",
    "encode_AG2(['acqic_etymd', 'csmcu_stscd'], ['uid'], train_df=train, test_df=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['conam'] = np.log(train['conam'])\n",
    "test['conam'] = np.log(test['conam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arctic Yen\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\decomposition\\sparse_pca.py:170: DeprecationWarning: normalize_components=False is a backward-compatible setting that implements a non-standard definition of sparse PCA. This compatibility mode will be removed in 0.22.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import FastICA, SparsePCA, TruncatedSVD\n",
    "\n",
    "cat_cols = ['cano', 'bacno', 'mcc', 'mchno', 'acqic', 'stocn', 'scity', 'csmcu', \n",
    "            'etymd', 'stscd', 'txn_info', 'contp', 'hcefg', 'iterm', 'ovrlt']\n",
    "n_comp = 5\n",
    "\n",
    "temp_train = train[cat_cols].copy()\n",
    "temp_test = test[cat_cols].copy()\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica_results_train = ica.fit_transform(temp_train)\n",
    "ica_results_test = ica.transform(temp_test)\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "tsvd_results_train = tsvd.fit_transform(temp_train)\n",
    "tsvd_results_test = tsvd.transform(temp_test)\n",
    "\n",
    "# SparsePCA\n",
    "spca = SparsePCA(n_components=n_comp, random_state=42)\n",
    "spca_results_train = spca.fit_transform(temp_train)\n",
    "spca_results_test = spca.transform(temp_test)\n",
    "\n",
    "for i in range(1, n_comp + 1):\n",
    "\n",
    "    train['tSVD_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tSVD_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica_results_test[:, i - 1]\n",
    "    \n",
    "    train['spca_' + str(i)] = ica_results_train[:, i - 1]\n",
    "    test['spca_' + str(i)] = ica_results_test[:, i - 1]\n",
    "\n",
    "del temp_train, temp_test, ica_results_train, ica_results_test, tsvd_results_train, tsvd_results_test, spca_results_train, spca_results_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaEncoder(object):\n",
    "        \n",
    "    def __init__(self, group):\n",
    "        \n",
    "        self.group = group\n",
    "        self.stats = None\n",
    "        \n",
    "    # get counts from df\n",
    "    def fit(self, df, target_col):\n",
    "        self.prior_mean = np.mean(df[target_col])\n",
    "        stats = df[[target_col, self.group]].groupby(self.group)\n",
    "        stats = stats.agg(['sum', 'count'])[target_col]    \n",
    "        stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n",
    "        stats.reset_index(level=0, inplace=True)           \n",
    "        self.stats = stats\n",
    "        \n",
    "    # extract posterior statistics\n",
    "    def transform(self, df, stat_type, N_min=1):\n",
    "        \n",
    "        df_stats = pd.merge(df[[self.group]], self.stats, how='left')\n",
    "        n = df_stats['n'].copy()\n",
    "        N = df_stats['N'].copy()\n",
    "        \n",
    "        # fill in missing\n",
    "        nan_indexs = np.isnan(n)\n",
    "        n[nan_indexs] = self.prior_mean\n",
    "        N[nan_indexs] = 1.0\n",
    "        \n",
    "        # prior parameters\n",
    "        N_prior = np.maximum(N_min-N, 0)\n",
    "        alpha_prior = self.prior_mean*N_prior\n",
    "        beta_prior = (1-self.prior_mean)*N_prior\n",
    "        \n",
    "        # posterior parameters\n",
    "        alpha = alpha_prior + n\n",
    "        beta =  beta_prior + N-n\n",
    "        \n",
    "        # calculate statistics\n",
    "        if stat_type=='mean':\n",
    "            num = alpha\n",
    "            dem = alpha+beta\n",
    "                    \n",
    "        elif stat_type=='mode':\n",
    "            num = alpha-1\n",
    "            dem = alpha+beta-2\n",
    "            \n",
    "        elif stat_type=='median':\n",
    "            num = alpha-1/3\n",
    "            dem = alpha+beta-2/3\n",
    "        \n",
    "        elif stat_type=='var':\n",
    "            num = alpha*beta\n",
    "            dem = (alpha+beta)**2*(alpha+beta+1)\n",
    "                    \n",
    "        elif stat_type=='skewness':\n",
    "            num = 2*(beta-alpha)*np.sqrt(alpha+beta+1)\n",
    "            dem = (alpha+beta+2)*np.sqrt(alpha*beta)\n",
    "\n",
    "        elif stat_type=='kurtosis':\n",
    "            num = 6*(alpha-beta)**2*(alpha+beta+1) - alpha*beta*(alpha+beta+2)\n",
    "            dem = alpha*beta*(alpha+beta+2)*(alpha+beta+3)\n",
    "            \n",
    "        # replace missing\n",
    "        value = num/dem\n",
    "        value[np.isnan(value)] = np.nanmedian(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['mchno','scity','acqic']\n",
    "\n",
    "#N_min = 1000  \n",
    "# encode variables\n",
    "for col in cat_columns:\n",
    "    all_data = pd.concat([train[col],test[col]])\n",
    "    N_min = (len(all_data.unique()))* 0.1\n",
    "    # fit encoder\n",
    "    be = BetaEncoder(col)\n",
    "    be.fit(train, 'fraud_ind')\n",
    "\n",
    "    # mean\n",
    "    train[col+'_beta'] = be.transform(train, 'mean', N_min)\n",
    "    test[col+'_beta']  = be.transform(test,  'mean', N_min)\n",
    "    \n",
    "    del all_data, N_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 498.61 Mb (57.6% reduction)\n",
      "Mem. usage decreased to 146.67 Mb (55.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "for df in [train, test]:\n",
    "    df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features = ['loctm', 'locdt', 'uid', 'uid2', 'insfg',\n",
    "                   'first_cano_dt', 'first_cano_dt_locdt_W_min_max', \n",
    "                   'locdt_W', 'locdt_M', #'loct_hour', \n",
    "                  ]\n",
    "\n",
    "features_columns = [col for col in list(train) if col not in remove_features]\n",
    "features_columns.remove('fraud_ind')\n",
    "target = 'fraud_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past_w_same_store\n"
     ]
    }
   ],
   "source": [
    "for col in features_columns:\n",
    "    if train[col].dtype=='O':\n",
    "        print(col)\n",
    "        train[col] = train[col].fillna('unseen_before_label')\n",
    "        test[col]  = test[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train[col] = train[col].astype(str)\n",
    "        test[col] = test[col].astype(str)\n",
    "        \n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(list(train[col])+list(test[col]))\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col]  = le.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW USING THE FOLLOWING 121 FEATURES.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg',\n",
       "       'etymd', 'flbmk', 'flg_3dsmk', 'hcefg', 'iterm', 'mcc', 'mchno',\n",
       "       'ovrlt', 'scity', 'stocn', 'stscd', 'loct_hour', 'txn_info',\n",
       "       'conam_check', 'cano_check', 'cano_mchno_fq', 'cano_stocn_fq',\n",
       "       'cano_csmcu_fq', 'cano_etymd_fq', 'cano_stscd_fq',\n",
       "       'cano_txn_info_fq', 'locdt_cano_count', 'loct_hour_cano_count',\n",
       "       'mchno_cano_fq', 'stocn_cano_fq', 'csmcu_cano_fq', 'etymd_cano_fq',\n",
       "       'stscd_cano_fq', 'txn_info_cano_fq', 'day_mchno_cano_fq',\n",
       "       'day_stocn_cano_fq', 'day_csmcu_cano_fq', 'past_w_same_store',\n",
       "       'conam_cano_mean', 'conam_cano_std', 'conam_bacno_mean',\n",
       "       'conam_bacno_std', 'conam_mcc_mean', 'conam_mcc_std',\n",
       "       'conam_mchno_mean', 'conam_mchno_std', 'conam_acqic_mean',\n",
       "       'conam_acqic_std', 'cano_FE', 'bacno_FE', 'mcc_FE', 'mchno_FE',\n",
       "       'acqic_FE', 'etymd_FE', 'stscd_FE', 'txn_info_FE', 'contp_FE',\n",
       "       'hcefg_FE', 'iterm_FE', 'etymd_target_mean', 'stscd_target_mean',\n",
       "       'txn_info_target_mean', 'mcc_mchno', 'scity_stocn', 'mchno_stocn',\n",
       "       'mcc_mchno_count_full', 'scity_stocn_count_full',\n",
       "       'mchno_stocn_count_full', 'csmcu_stscd', 'acqic_etymd',\n",
       "       'acqic_etymd_FE', 'csmcu_stscd_FE', 'stscd_etymd',\n",
       "       'txn_info_etymd', 'uid_FE', 'conam_uid_mean', 'conam_uid_std',\n",
       "       'mcc_mchno_uid_mean', 'mcc_mchno_uid_std', 'scity_stocn_uid_mean',\n",
       "       'scity_stocn_uid_std', 'mchno_stocn_uid_mean',\n",
       "       'mchno_stocn_uid_std', 'etymd_uid_mean', 'stscd_uid_mean',\n",
       "       'txn_info_uid_mean', 'uid_conam_ct', 'uid_scity_stocn_ct',\n",
       "       'uid_mcc_mchno_ct', 'uid_mchno_stocn_ct', 'uid2_FE',\n",
       "       'conam_uid2_mean', 'conam_uid2_std', 'acqic_etymd_uid2_mean',\n",
       "       'acqic_etymd_uid2_std', 'csmcu_stscd_uid2_mean',\n",
       "       'csmcu_stscd_uid2_std', 'stscd_etymd_uid_std',\n",
       "       'txn_info_etymd_uid_std', 'uid_acqic_etymd_ct',\n",
       "       'uid_csmcu_stscd_ct', 'tSVD_1', 'ica_1', 'spca_1', 'tSVD_2',\n",
       "       'ica_2', 'spca_2', 'tSVD_3', 'ica_3', 'spca_3', 'tSVD_4', 'ica_4',\n",
       "       'spca_4', 'tSVD_5', 'ica_5', 'spca_5', 'mchno_beta', 'scity_beta',\n",
       "       'acqic_beta'], dtype='<U22')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('NOW USING THE FOLLOWING',len(features_columns),'FEATURES.')\n",
    "np.array(features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train[train['locdt_M']==3]\n",
    "train = train[train['locdt_M']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of y_train data : (533805,)\n",
      "Size of X_train data : (533805, 121)\n",
      "Size of test data : (490546, 121)\n"
     ]
    }
   ],
   "source": [
    "y = train['fraud_ind']\n",
    "X = train[features_columns]\n",
    "\n",
    "X_test_target = pd.DataFrame()\n",
    "X_test_target['fraud_ind'] = test['fraud_ind']\n",
    "X_test = test[features_columns]\n",
    "\n",
    "print (\"Size of y_train data : {}\" .format(y.shape))\n",
    "print (\"Size of X_train data : {}\" .format(X.shape))\n",
    "print (\"Size of X_test data : {}\" .format(X_test.shape))\n",
    "\n",
    "#del train, test\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective': 'binary',\n",
    "          'boosting_type': 'gbdt',\n",
    "          'metric': 'auc',\n",
    "          'learning_rate': 0.005,\n",
    "          'max_depth': -1,\n",
    "          'tree_learner':'serial',\n",
    "          'colsample_bytree': 0.7,\n",
    "          'subsample':0.7,\n",
    "          'n_estimators':3000,\n",
    "          'verbosity': -1,\n",
    "          'seed': SEED,\n",
    "          #'early_stopping_rounds':100,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column: acqic\n",
      "Predict AUC: 0.5\n",
      "column: bacno\n",
      "Predict AUC: 0.5\n",
      "column: cano\n",
      "Predict AUC: 0.5\n",
      "column: conam\n",
      "Predict AUC: 0.5\n",
      "column: contp\n",
      "Predict AUC: 0.5\n",
      "column: csmcu\n",
      "Predict AUC: 0.5\n",
      "column: ecfg\n",
      "Predict AUC: 0.5\n",
      "column: etymd\n",
      "Predict AUC: 0.5\n",
      "column: flbmk\n",
      "Predict AUC: 0.5\n",
      "column: flg_3dsmk\n",
      "Predict AUC: 0.5\n",
      "column: hcefg\n",
      "Predict AUC: 0.5\n",
      "column: iterm\n",
      "Predict AUC: 0.5\n",
      "column: mcc\n",
      "Predict AUC: 0.5\n",
      "column: mchno\n",
      "Predict AUC: 0.5\n",
      "column: ovrlt\n",
      "Predict AUC: 0.5\n",
      "column: scity\n",
      "Predict AUC: 0.5319366448884457\n",
      "column: stocn\n",
      "Predict AUC: 0.5\n",
      "column: stscd\n",
      "Predict AUC: 0.5\n",
      "column: loct_hour\n",
      "Predict AUC: 0.5\n",
      "column: txn_info\n",
      "Predict AUC: 0.5\n",
      "column: conam_check\n",
      "Predict AUC: 0.5\n",
      "column: cano_check\n",
      "Predict AUC: 0.5\n",
      "column: cano_mchno_fq\n",
      "Predict AUC: 0.5\n",
      "column: cano_stocn_fq\n",
      "Predict AUC: 0.5\n",
      "column: cano_csmcu_fq\n",
      "Predict AUC: 0.5\n",
      "column: cano_etymd_fq\n"
     ]
    }
   ],
   "source": [
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X.shape[0])\n",
    "auc_score = 0\n",
    "columns = list(X.columns)\n",
    "bad_feature = []\n",
    "\n",
    "for col in columns:\n",
    "\n",
    "    print(f\"column: {col}\")\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "    X_train = np.array(X[col]).reshape((-1,1))\n",
    "    X_vaild = np.array(y).reshape((-1,1))\n",
    "    X_pred = np.array(X_test[col]).reshape((-1,1))\n",
    "    clf.fit(X_train, X_vaild)\n",
    "    \n",
    "    \n",
    "    y_preds = clf.predict(X_pred)\n",
    "    \n",
    "    X_test_target['pred'] = y_preds\n",
    "    \n",
    "    auc_score = roc_auc_score(X_test_target['fraud_ind'].values, X_test_target['pred'])\n",
    "    print(f\"Predict AUC: {auc_score}\")\n",
    "    \n",
    "    if auc_score < 0.5:\n",
    "        bad_feature.append(col)\n",
    "    \n",
    "    del X_train, X_vaild, X_pred\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bad_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
